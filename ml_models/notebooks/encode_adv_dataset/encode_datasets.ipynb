{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bebdd8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# Add paths so Python can find the event_log_loader module when unpickling\n",
    "sys.path.insert(0, '..')\n",
    "sys.path.insert(0, '../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e61b915",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helpdesk\n",
    "# data_path='../../../perturbed_data/helpdesk/redo_pert.pkl'\n",
    "# result_path='../../../encoded_data/helpdesk/redo_activity.pkl'\n",
    "# encoder_path='../../../encoded_data/data_encoder/helpdesk_encoder_decoder.pkl'\n",
    "# properties_path='../../../encoded_data/data_encoder/helpdesk_event_log_properties.pkl'\n",
    "\n",
    "#sepsis\n",
    "# data_path='../../../perturbed_data/sepsis/redo_activity.pkl'\n",
    "# result_path='../../../encoded_data/sepsis/redo_activity.pkl'\n",
    "# encoder_path='../../../encoded_data/data_encoder/sepsis_encoder_decoder.pkl'\n",
    "# properties_path='../../../encoded_data/data_encoder/sepsis_event_log_properties.pkl'\n",
    "\n",
    "#BPIC17\n",
    "data_path='../../../perturbed_data/BPIC17/redo_pert.pkl'\n",
    "result_path='../../../encoded_data/BPIC17/redo_activity.pkl'\n",
    "encoder_path='../../../encoded_data/data_encoder/BPIC17_encoder_decoder.pkl'\n",
    "properties_path='../../../encoded_data/data_encoder/BPIC17_event_log_properties.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7ba9668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_single_dataframe(df, encoder_decoder, case_name_col, case_id_value):\n",
    "    \"\"\"\n",
    "    Encode a single DataFrame (prefix or suffix) into tensor format.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to encode (prefix or suffix)\n",
    "        encoder_decoder: TensorEncoderDecoder instance\n",
    "        case_name_col: Name of the case column\n",
    "        case_id_value: Value for the case ID (to ensure proper grouping)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (categorical_tensors, numerical_tensors) where each is a list\n",
    "        of tensors with shape (1, window_size)\n",
    "    \"\"\"\n",
    "    # Ensure the DataFrame has the case_name column for proper encoding\n",
    "    df_copy = df.copy()\n",
    "    if case_name_col not in df_copy.columns:\n",
    "        df_copy[case_name_col] = case_id_value\n",
    "    else:\n",
    "        # Ensure all rows have the same case_id\n",
    "        df_copy[case_name_col] = case_id_value\n",
    "    \n",
    "    # Encode categorical columns\n",
    "    cat_tensors = []\n",
    "    for col in encoder_decoder.categorical_columns:\n",
    "        if col not in df_copy.columns:\n",
    "            # Create zero tensor if column missing\n",
    "            cat_tensors.append(torch.zeros((1, encoder_decoder.window_size), dtype=torch.long))\n",
    "            continue\n",
    "            \n",
    "        # Get values for this column\n",
    "        case_values = np.array(df_copy[[col]], dtype=object)\n",
    "        # Transform using the fitted encoder\n",
    "        case_values_enc = encoder_decoder.categorical_encoders[col].transform(case_values) + 1\n",
    "        \n",
    "        # Pad to window_size\n",
    "        padded = encoder_decoder.pad_to_window_size(case_values_enc)\n",
    "        # Convert to tensor and add batch dimension\n",
    "        tensor = torch.tensor(padded, dtype=torch.long).squeeze(-1)  # shape: (window_size,)\n",
    "        tensor = tensor.unsqueeze(0)  # shape: (1, window_size)\n",
    "        cat_tensors.append(tensor)\n",
    "    \n",
    "    # Encode continuous columns\n",
    "    num_tensors = []\n",
    "    for col in encoder_decoder.continuous_columns + encoder_decoder.continuous_positive_columns:\n",
    "        if col not in df_copy.columns:\n",
    "            # Create zero tensor if column missing\n",
    "            num_tensors.append(torch.zeros((1, encoder_decoder.window_size), dtype=torch.float32))\n",
    "            continue\n",
    "            \n",
    "        # Get values for this column\n",
    "        case_values = df_copy[[col]].values  # shape (n, 1)\n",
    "        # Impute and transform\n",
    "        case_values_imputed = encoder_decoder.continuous_imputers[col].transform(case_values)\n",
    "        case_values_enc = encoder_decoder.continuous_encoders[col].transform(case_values_imputed)\n",
    "        \n",
    "        # Pad to window_size\n",
    "        padded = encoder_decoder.pad_to_window_size(case_values_enc)\n",
    "        # Convert to tensor and add batch dimension\n",
    "        tensor = torch.tensor(padded, dtype=torch.float32).squeeze(-1)  # shape: (window_size,)\n",
    "        tensor = tensor.unsqueeze(0)  # shape: (1, window_size)\n",
    "        num_tensors.append(tensor)\n",
    "    \n",
    "    return (cat_tensors, num_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8310ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chair/henryks_students/leon_urny/Robustness-in-suffix-prediction/.venv/lib64/python3.13/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator OrdinalEncoder from version 1.7.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/chair/henryks_students/leon_urny/Robustness-in-suffix-prediction/.venv/lib64/python3.13/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator SimpleImputer from version 1.7.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/chair/henryks_students/leon_urny/Robustness-in-suffix-prediction/.venv/lib64/python3.13/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.7.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8734795c2fb449e6901ec6bf3d63b9d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding data:   0%|          | 0/171611 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 171611 prefix/suffix pairs\n"
     ]
    }
   ],
   "source": [
    "# Load the trained encoder_decoder\n",
    "encoder_decoder = torch.load(encoder_path, weights_only=False)\n",
    "#load data\n",
    "data = torch.load(data_path, weights_only=False)\n",
    "\n",
    "with open(properties_path, 'rb') as f:\n",
    "    props = pickle.load(f)\n",
    "\n",
    "# Encode all prefix/suffix pairs\n",
    "encoded_data = {}\n",
    "for (case_id, prefix_len), (prefix_df, suffix_df) in tqdm(data.items(), desc=\"Encoding data\"):\n",
    "    # Encode prefix\n",
    "    encoded_prefix = encode_single_dataframe(\n",
    "        prefix_df, encoder_decoder, props[\"case_name\"], case_id\n",
    "    )\n",
    "    # Encode suffix\n",
    "    encoded_suffix = encode_single_dataframe(\n",
    "        suffix_df, encoder_decoder, props[\"case_name\"], case_id\n",
    "    )\n",
    "    # Store encoded pair\n",
    "    encoded_data[(case_id, prefix_len)] = (encoded_prefix, encoded_suffix)\n",
    "\n",
    "print(f\"Encoded {len(encoded_data)} prefix/suffix pairs\")\n",
    "\n",
    "torch.save(encoded_data, result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324ecfd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
