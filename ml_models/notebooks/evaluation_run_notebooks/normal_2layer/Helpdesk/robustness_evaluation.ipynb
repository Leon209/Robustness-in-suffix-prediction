{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import importlib\n",
        "import sys\n",
        "import torch\n",
        "import pickle\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "import concurrent.futures\n",
        "from functools import partial\n",
        "\n",
        "sys.path.insert(0, '..')\n",
        "sys.path.insert(0, '../..')\n",
        "sys.path.insert(0, '../../..')\n",
        "sys.path.insert(0, '../../../..')\n",
        "sys.path.insert(0, '../../../../..')\n",
        "\n",
        "from model.dropout_uncertainty_enc_dec_LSTM.dropout_uncertainty_model import DropoutUncertaintyEncoderDecoderLSTM\n",
        "from evaluation.probabilistic_evaluation import ProbabilisticEvaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data set categories:  ([('Activity', 16, {'Assign seriousness': 1, 'Closed': 2, 'Create SW anomaly': 3, 'DUPLICATE': 4, 'EOS': 5, 'INVALID': 6, 'Insert ticket': 7, 'RESOLVED': 8, 'Require upgrade': 9, 'Resolve SW anomaly': 10, 'Resolve ticket': 11, 'Schedule intervention': 12, 'Take in charge ticket': 13, 'VERIFIED': 14, 'Wait': 15}), ('Resource', 24, {'EOS': 1, 'Value 1': 2, 'Value 10': 3, 'Value 11': 4, 'Value 12': 5, 'Value 13': 6, 'Value 14': 7, 'Value 15': 8, 'Value 16': 9, 'Value 17': 10, 'Value 18': 11, 'Value 19': 12, 'Value 2': 13, 'Value 20': 14, 'Value 21': 15, 'Value 22': 16, 'Value 3': 17, 'Value 4': 18, 'Value 5': 19, 'Value 6': 20, 'Value 7': 21, 'Value 8': 22, 'Value 9': 23}), ('Variant index', 166, {'1.0': 1, '10.0': 2, '100.0': 3, '101.0': 4, '102.0': 5, '103.0': 6, '104.0': 7, '105.0': 8, '106.0': 9, '107.0': 10, '108.0': 11, '109.0': 12, '11.0': 13, '110.0': 14, '111.0': 15, '112.0': 16, '113.0': 17, '114.0': 18, '12.0': 19, '13.0': 20, '14.0': 21, '15.0': 22, '16.0': 23, '168.0': 24, '169.0': 25, '17.0': 26, '170.0': 27, '171.0': 28, '172.0': 29, '173.0': 30, '174.0': 31, '175.0': 32, '176.0': 33, '177.0': 34, '178.0': 35, '179.0': 36, '18.0': 37, '180.0': 38, '181.0': 39, '182.0': 40, '183.0': 41, '184.0': 42, '185.0': 43, '186.0': 44, '187.0': 45, '188.0': 46, '189.0': 47, '19.0': 48, '190.0': 49, '191.0': 50, '192.0': 51, '193.0': 52, '194.0': 53, '195.0': 54, '196.0': 55, '197.0': 56, '198.0': 57, '199.0': 58, '2.0': 59, '20.0': 60, '200.0': 61, '201.0': 62, '202.0': 63, '203.0': 64, '204.0': 65, '205.0': 66, '206.0': 67, '207.0': 68, '208.0': 69, '209.0': 70, '21.0': 71, '210.0': 72, '211.0': 73, '212.0': 74, '213.0': 75, '214.0': 76, '215.0': 77, '216.0': 78, '217.0': 79, '218.0': 80, '219.0': 81, '22.0': 82, '220.0': 83, '221.0': 84, '222.0': 85, '223.0': 86, '224.0': 87, '225.0': 88, '226.0': 89, '23.0': 90, '24.0': 91, '25.0': 92, '26.0': 93, '27.0': 94, '28.0': 95, '29.0': 96, '3.0': 97, '30.0': 98, '31.0': 99, '32.0': 100, '33.0': 101, '34.0': 102, '35.0': 103, '36.0': 104, '37.0': 105, '38.0': 106, '39.0': 107, '4.0': 108, '40.0': 109, '41.0': 110, '42.0': 111, '43.0': 112, '44.0': 113, '45.0': 114, '46.0': 115, '47.0': 116, '48.0': 117, '49.0': 118, '5.0': 119, '50.0': 120, '52.0': 121, '53.0': 122, '54.0': 123, '55.0': 124, '56.0': 125, '57.0': 126, '58.0': 127, '59.0': 128, '6.0': 129, '60.0': 130, '62.0': 131, '63.0': 132, '64.0': 133, '65.0': 134, '66.0': 135, '67.0': 136, '68.0': 137, '69.0': 138, '7.0': 139, '70.0': 140, '71.0': 141, '72.0': 142, '74.0': 143, '75.0': 144, '76.0': 145, '77.0': 146, '79.0': 147, '8.0': 148, '80.0': 149, '82.0': 150, '83.0': 151, '84.0': 152, '85.0': 153, '86.0': 154, '87.0': 155, '88.0': 156, '9.0': 157, '90.0': 158, '91.0': 159, '92.0': 160, '93.0': 161, '94.0': 162, '95.0': 163, '99.0': 164, nan: 165}), ('seriousness', 3, {'EOS': 1, 'Value 1': 2}), ('customer', 358, {'EOS': 1, 'Value 1': 2, 'Value 10': 3, 'Value 100': 4, 'Value 101': 5, 'Value 102': 6, 'Value 103': 7, 'Value 104': 8, 'Value 105': 9, 'Value 106': 10, 'Value 107': 11, 'Value 108': 12, 'Value 11': 13, 'Value 110': 14, 'Value 112': 15, 'Value 113': 16, 'Value 115': 17, 'Value 116': 18, 'Value 117': 19, 'Value 118': 20, 'Value 119': 21, 'Value 12': 22, 'Value 120': 23, 'Value 121': 24, 'Value 122': 25, 'Value 123': 26, 'Value 124': 27, 'Value 125': 28, 'Value 126': 29, 'Value 127': 30, 'Value 128': 31, 'Value 129': 32, 'Value 13': 33, 'Value 130': 34, 'Value 131': 35, 'Value 132': 36, 'Value 133': 37, 'Value 134': 38, 'Value 135': 39, 'Value 136': 40, 'Value 137': 41, 'Value 138': 42, 'Value 139': 43, 'Value 14': 44, 'Value 140': 45, 'Value 141': 46, 'Value 142': 47, 'Value 143': 48, 'Value 144': 49, 'Value 145': 50, 'Value 146': 51, 'Value 147': 52, 'Value 148': 53, 'Value 149': 54, 'Value 15': 55, 'Value 150': 56, 'Value 151': 57, 'Value 152': 58, 'Value 153': 59, 'Value 154': 60, 'Value 155': 61, 'Value 156': 62, 'Value 157': 63, 'Value 158': 64, 'Value 159': 65, 'Value 16': 66, 'Value 160': 67, 'Value 161': 68, 'Value 162': 69, 'Value 163': 70, 'Value 164': 71, 'Value 165': 72, 'Value 166': 73, 'Value 167': 74, 'Value 168': 75, 'Value 169': 76, 'Value 17': 77, 'Value 170': 78, 'Value 171': 79, 'Value 172': 80, 'Value 173': 81, 'Value 174': 82, 'Value 175': 83, 'Value 176': 84, 'Value 177': 85, 'Value 178': 86, 'Value 179': 87, 'Value 18': 88, 'Value 180': 89, 'Value 181': 90, 'Value 182': 91, 'Value 183': 92, 'Value 184': 93, 'Value 185': 94, 'Value 186': 95, 'Value 187': 96, 'Value 188': 97, 'Value 189': 98, 'Value 19': 99, 'Value 190': 100, 'Value 191': 101, 'Value 192': 102, 'Value 193': 103, 'Value 194': 104, 'Value 195': 105, 'Value 196': 106, 'Value 197': 107, 'Value 198': 108, 'Value 199': 109, 'Value 2': 110, 'Value 20': 111, 'Value 200': 112, 'Value 201': 113, 'Value 202': 114, 'Value 203': 115, 'Value 204': 116, 'Value 205': 117, 'Value 206': 118, 'Value 207': 119, 'Value 208': 120, 'Value 209': 121, 'Value 21': 122, 'Value 210': 123, 'Value 211': 124, 'Value 212': 125, 'Value 213': 126, 'Value 214': 127, 'Value 215': 128, 'Value 216': 129, 'Value 217': 130, 'Value 218': 131, 'Value 219': 132, 'Value 22': 133, 'Value 220': 134, 'Value 221': 135, 'Value 222': 136, 'Value 223': 137, 'Value 224': 138, 'Value 225': 139, 'Value 226': 140, 'Value 227': 141, 'Value 228': 142, 'Value 229': 143, 'Value 23': 144, 'Value 230': 145, 'Value 231': 146, 'Value 232': 147, 'Value 233': 148, 'Value 234': 149, 'Value 235': 150, 'Value 236': 151, 'Value 237': 152, 'Value 238': 153, 'Value 239': 154, 'Value 24': 155, 'Value 240': 156, 'Value 241': 157, 'Value 242': 158, 'Value 243': 159, 'Value 244': 160, 'Value 245': 161, 'Value 246': 162, 'Value 247': 163, 'Value 248': 164, 'Value 249': 165, 'Value 25': 166, 'Value 250': 167, 'Value 251': 168, 'Value 252': 169, 'Value 253': 170, 'Value 254': 171, 'Value 255': 172, 'Value 256': 173, 'Value 259': 174, 'Value 26': 175, 'Value 260': 176, 'Value 261': 177, 'Value 262': 178, 'Value 263': 179, 'Value 266': 180, 'Value 267': 181, 'Value 27': 182, 'Value 270': 183, 'Value 271': 184, 'Value 272': 185, 'Value 273': 186, 'Value 275': 187, 'Value 278': 188, 'Value 28': 189, 'Value 282': 190, 'Value 283': 191, 'Value 284': 192, 'Value 286': 193, 'Value 287': 194, 'Value 29': 195, 'Value 290': 196, 'Value 292': 197, 'Value 293': 198, 'Value 295': 199, 'Value 296': 200, 'Value 299': 201, 'Value 3': 202, 'Value 30': 203, 'Value 300': 204, 'Value 301': 205, 'Value 302': 206, 'Value 303': 207, 'Value 304': 208, 'Value 306': 209, 'Value 307': 210, 'Value 308': 211, 'Value 309': 212, 'Value 31': 213, 'Value 311': 214, 'Value 312': 215, 'Value 316': 216, 'Value 317': 217, 'Value 318': 218, 'Value 319': 219, 'Value 32': 220, 'Value 320': 221, 'Value 321': 222, 'Value 322': 223, 'Value 325': 224, 'Value 328': 225, 'Value 33': 226, 'Value 331': 227, 'Value 333': 228, 'Value 334': 229, 'Value 335': 230, 'Value 338': 231, 'Value 34': 232, 'Value 342': 233, 'Value 343': 234, 'Value 344': 235, 'Value 345': 236, 'Value 346': 237, 'Value 347': 238, 'Value 348': 239, 'Value 349': 240, 'Value 35': 241, 'Value 350': 242, 'Value 351': 243, 'Value 352': 244, 'Value 353': 245, 'Value 354': 246, 'Value 355': 247, 'Value 356': 248, 'Value 357': 249, 'Value 358': 250, 'Value 359': 251, 'Value 36': 252, 'Value 360': 253, 'Value 361': 254, 'Value 362': 255, 'Value 363': 256, 'Value 364': 257, 'Value 365': 258, 'Value 366': 259, 'Value 367': 260, 'Value 368': 261, 'Value 369': 262, 'Value 37': 263, 'Value 370': 264, 'Value 371': 265, 'Value 372': 266, 'Value 373': 267, 'Value 374': 268, 'Value 375': 269, 'Value 376': 270, 'Value 377': 271, 'Value 378': 272, 'Value 379': 273, 'Value 38': 274, 'Value 380': 275, 'Value 381': 276, 'Value 382': 277, 'Value 383': 278, 'Value 384': 279, 'Value 385': 280, 'Value 386': 281, 'Value 387': 282, 'Value 388': 283, 'Value 389': 284, 'Value 39': 285, 'Value 390': 286, 'Value 391': 287, 'Value 392': 288, 'Value 393': 289, 'Value 394': 290, 'Value 395': 291, 'Value 396': 292, 'Value 397': 293, 'Value 4': 294, 'Value 40': 295, 'Value 41': 296, 'Value 42': 297, 'Value 43': 298, 'Value 44': 299, 'Value 45': 300, 'Value 46': 301, 'Value 47': 302, 'Value 48': 303, 'Value 49': 304, 'Value 5': 305, 'Value 50': 306, 'Value 51': 307, 'Value 52': 308, 'Value 53': 309, 'Value 54': 310, 'Value 55': 311, 'Value 56': 312, 'Value 57': 313, 'Value 58': 314, 'Value 59': 315, 'Value 6': 316, 'Value 60': 317, 'Value 61': 318, 'Value 62': 319, 'Value 63': 320, 'Value 64': 321, 'Value 65': 322, 'Value 66': 323, 'Value 67': 324, 'Value 68': 325, 'Value 69': 326, 'Value 7': 327, 'Value 70': 328, 'Value 71': 329, 'Value 72': 330, 'Value 73': 331, 'Value 74': 332, 'Value 75': 333, 'Value 76': 334, 'Value 77': 335, 'Value 78': 336, 'Value 79': 337, 'Value 8': 338, 'Value 80': 339, 'Value 81': 340, 'Value 82': 341, 'Value 83': 342, 'Value 85': 343, 'Value 86': 344, 'Value 87': 345, 'Value 88': 346, 'Value 89': 347, 'Value 9': 348, 'Value 90': 349, 'Value 91': 350, 'Value 92': 351, 'Value 93': 352, 'Value 94': 353, 'Value 96': 354, 'Value 97': 355, 'Value 98': 356, 'Value 99': 357}), ('product', 23, {'EOS': 1, 'Value 1': 2, 'Value 10': 3, 'Value 11': 4, 'Value 12': 5, 'Value 13': 6, 'Value 14': 7, 'Value 15': 8, 'Value 16': 9, 'Value 17': 10, 'Value 18': 11, 'Value 19': 12, 'Value 2': 13, 'Value 20': 14, 'Value 21': 15, 'Value 3': 16, 'Value 4': 17, 'Value 5': 18, 'Value 6': 19, 'Value 7': 20, 'Value 8': 21, 'Value 9': 22}), ('responsible_section', 9, {'EOS': 1, 'Value 1': 2, 'Value 2': 3, 'Value 3': 4, 'Value 4': 5, 'Value 5': 6, 'Value 6': 7, 'Value 7': 8}), ('seriousness_2', 6, {'EOS': 1, 'Value 1': 2, 'Value 2': 3, 'Value 3': 4, 'Value 4': 5}), ('service_level', 6, {'EOS': 1, 'Value 1': 2, 'Value 2': 3, 'Value 3': 4, 'Value 4': 5}), ('service_type', 6, {'EOS': 1, 'Value 1': 2, 'Value 2': 3, 'Value 3': 4, 'Value 4': 5}), ('support_section', 8, {'EOS': 1, 'Value 1': 2, 'Value 2': 3, 'Value 3': 4, 'Value 4': 5, 'Value 5': 6, 'Value 6': 7}), ('workgroup', 6, {'EOS': 1, 'Value 1': 2, 'Value 2': 3, 'Value 3': 4, 'Value 4': 5})], [('case_elapsed_time', 1, {}), ('event_elapsed_time', 1, {}), ('day_in_week', 1, {}), ('seconds_in_day', 1, {})])\n",
            "Encoder input features:  [['Activity', 'Resource', 'Variant index', 'seriousness', 'customer', 'product', 'responsible_section', 'seriousness_2', 'service_level', 'service_type', 'support_section', 'workgroup'], ['case_elapsed_time', 'event_elapsed_time', 'day_in_week', 'seconds_in_day']]\n",
            "Decoder input+output features:  [['Activity', 'Resource'], ['case_elapsed_time', 'event_elapsed_time']]\n",
            "\n",
            "\n",
            "Sequence length of decoder output:  4\n",
            "\n",
            "\n",
            "Cells hidden size:  128\n",
            "Number of LSTM layer:  4\n",
            "Dropout rate:  0.1\n",
            "\n",
            "\n",
            "Encoder number of labels for each input feature (categorical, numerical):  [[16, 24, 166, 3, 358, 23, 9, 6, 6, 6, 8, 6], [1, 1, 1, 1]]\n",
            "Encoder indices of tensors in dataset used as input:  [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [0, 1, 2, 3]]\n",
            "Embeddings encoder:  ModuleList(\n",
            "  (0): Embedding(16, 8)\n",
            "  (1): Embedding(24, 9)\n",
            "  (2): Embedding(166, 28)\n",
            "  (3): Embedding(3, 3)\n",
            "  (4): Embedding(358, 43)\n",
            "  (5): Embedding(23, 9)\n",
            "  (6): Embedding(9, 5)\n",
            "  (7-9): 3 x Embedding(6, 4)\n",
            "  (10): Embedding(8, 5)\n",
            "  (11): Embedding(6, 4)\n",
            ")\n",
            "Total embedding feature size encoder:  126\n",
            "Total numerical feature size encoder:  4\n",
            "Input feature size encoder:  130\n",
            "Encoder initialized! \n",
            "\n",
            "Decoder label values size for each categorical input feature:  [16, 24]\n",
            "Decoder label values size for each numerical input feature:  [1, 1]\n",
            "Decoder indices of tensors in dataset used as input:  [[0, 1], [0, 1]]\n",
            "Embeddings decoder:  ModuleList(\n",
            "  (0): Embedding(16, 8)\n",
            "  (1): Embedding(24, 9)\n",
            ")\n",
            "Total embedding feature size decoder:  17\n",
            "Total numerical feature size decoder:  2\n",
            "Input feature size decoder:  19\n",
            "Output feature list of dicts (featue name, feature output size) of decoder:  [{'Activity': 16, 'Resource': 24}, {'case_elapsed_time': 1, 'event_elapsed_time': 1}]\n",
            "Decoder initialized! \n",
            "\n",
            "Output feature list of dicts (featue name, tensor index in dataset) of decoder:  [{'Activity': 0, 'Resource': 1}, {'case_elapsed_time': 0, 'event_elapsed_time': 1}]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/chair/henryks_students/leon_urny/Robustness-in-suffix-prediction/.venv/lib64/python3.13/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator OrdinalEncoder from version 1.7.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/home/chair/henryks_students/leon_urny/Robustness-in-suffix-prediction/.venv/lib64/python3.13/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator SimpleImputer from version 1.7.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/home/chair/henryks_students/leon_urny/Robustness-in-suffix-prediction/.venv/lib64/python3.13/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.7.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/home/chair/henryks_students/leon_urny/Robustness-in-suffix-prediction/.venv/lib64/python3.13/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator OrdinalEncoder from version 1.7.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/home/chair/henryks_students/leon_urny/Robustness-in-suffix-prediction/.venv/lib64/python3.13/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator SimpleImputer from version 1.7.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/home/chair/henryks_students/leon_urny/Robustness-in-suffix-prediction/.venv/lib64/python3.13/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.7.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original dataset loaded: 5192 cases\n",
            "Perturbed dataset loaded: 5192 cases\n"
          ]
        }
      ],
      "source": [
        "# Load model\n",
        "file_path_model = '../../../training_variational_dropout/Helpdesk/Helpdesk_setting_2.pkl'\n",
        "output_dir = '../../../../../evaluation_results/robustness/Helpdesk/random_event_attack/'\n",
        "model = DropoutUncertaintyEncoderDecoderLSTM.load(file_path_model, dropout=0.1)\n",
        "\n",
        "# Load datasets\n",
        "file_path_original = '../../../../../encoded_data/helpdesk_all_5_test.pkl'\n",
        "file_path_perturbed = '../../../../../encoded_data/helpdesk_small_perturbations_test.pkl'\n",
        "file_path_redo_activity = '../../../../../encoded_data/helpdesk/val.pkl'\n",
        "file_path_redo_activity_pert = '../../../../../encoded_data/helpdesk/random_event_attack.pkl'\n",
        "\n",
        "\n",
        "original_dataset = torch.load(file_path_original, weights_only=False)\n",
        "perturbed_dataset = torch.load(file_path_perturbed, weights_only=False)\n",
        "redo_activity_dataset = torch.load(file_path_redo_activity, weights_only=False)\n",
        "redo_activity_pert_dataset = torch.load(file_path_redo_activity_pert, weights_only=False)\n",
        "\n",
        "\n",
        "print(f\"Original dataset loaded: {len(original_dataset)} cases\")\n",
        "print(f\"Perturbed dataset loaded: {len(perturbed_dataset)} cases\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ProbabilisticEvaluation instances created\n"
          ]
        }
      ],
      "source": [
        "# Create evaluation instances (NON-RANDOM ORDER)\n",
        "eval_original = ProbabilisticEvaluation(\n",
        "    model, original_dataset,\n",
        "    concept_name='Activity',\n",
        "    num_processes=1, #16\n",
        "    growing_num_values=['case_elapsed_time'],\n",
        "    samples_per_case=100,\n",
        "    sample_argmax=False,\n",
        "    use_variance_cat=True,\n",
        "    use_variance_num=True,\n",
        "    all_cat=['Activity', 'Resource'],\n",
        "    all_num=['case_elapsed_time', 'event_elapsed_time'],\n",
        "    dataset_predefined_prefixes=redo_activity_dataset\n",
        ")\n",
        "\n",
        "eval_perturbed = ProbabilisticEvaluation(\n",
        "    model, perturbed_dataset,\n",
        "    concept_name='Activity',\n",
        "    num_processes=1,#16\n",
        "    growing_num_values=['case_elapsed_time'],\n",
        "    samples_per_case=100,\n",
        "    sample_argmax=False,\n",
        "    use_variance_cat=True,\n",
        "    use_variance_num=True,\n",
        "    all_cat=['Activity', 'Resource'],\n",
        "    all_num=['case_elapsed_time', 'event_elapsed_time'],\n",
        "    dataset_predefined_prefixes=redo_activity_pert_dataset\n",
        ")\n",
        "\n",
        "print(\"ProbabilisticEvaluation instances created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Robustness metrics module imported\n"
          ]
        }
      ],
      "source": [
        "# Import robustness metrics module\n",
        "import robustness.evaluator.robustness_metrics\n",
        "#importlib.reload(robustness.robustness_metrics)\n",
        "from robustness.evaluator.robustness_metrics import save_chunk\n",
        "\n",
        "print(\"Robustness metrics module imported\")\n",
        "\n",
        "# Helper functions for filtering predictions and calculating remaining time\n",
        "def filter_prediction_events(prediction_list, concept_name='Activity'):\n",
        "    \"\"\"Filter prediction events to only include concept:name and case_elapsed_time\"\"\"\n",
        "    if prediction_list is None:\n",
        "        return None\n",
        "    filtered = []\n",
        "    for event in prediction_list:\n",
        "        if not isinstance(event, dict):\n",
        "            continue\n",
        "        filtered_event = {}\n",
        "        if concept_name in event:\n",
        "            filtered_event[concept_name] = event[concept_name]\n",
        "        if 'case_elapsed_time' in event:\n",
        "            filtered_event['case_elapsed_time'] = event['case_elapsed_time']\n",
        "        filtered.append(filtered_event)\n",
        "    return filtered\n",
        "\n",
        "def calculate_remaining_time(prefix, prediction, concept_name='Activity'):\n",
        "    \"\"\"Calculate remaining time from prefix and prediction\"\"\"\n",
        "    if not prefix or not prediction:\n",
        "        return None\n",
        "    if not isinstance(prefix[-1], dict) or 'case_elapsed_time' not in prefix[-1]:\n",
        "        return None\n",
        "    if not isinstance(prediction[-1], dict) or 'case_elapsed_time' not in prediction[-1]:\n",
        "        return None\n",
        "    current_time = prefix[-1]['case_elapsed_time']\n",
        "    final_time = prediction[-1]['case_elapsed_time']\n",
        "    return final_time - current_time\n",
        "\n",
        "def calculate_sampled_remaining_times(prefix, predicted_suffixes, concept_name='Activity'):\n",
        "    \"\"\"Calculate remaining time for each sample in predicted_suffixes\"\"\"\n",
        "    if not prefix or not predicted_suffixes:\n",
        "        return None\n",
        "    if not isinstance(prefix[-1], dict) or 'case_elapsed_time' not in prefix[-1]:\n",
        "        return None\n",
        "    current_time = prefix[-1]['case_elapsed_time']\n",
        "    \n",
        "    remaining_times = []\n",
        "    for sample in predicted_suffixes:\n",
        "        if not sample or not isinstance(sample[-1], dict) or 'case_elapsed_time' not in sample[-1]:\n",
        "            remaining_times.append(None)\n",
        "        else:\n",
        "            final_time = sample[-1]['case_elapsed_time']\n",
        "            remaining_times.append(final_time - current_time)\n",
        "    return remaining_times\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting prefix/suffix pairs from predefined datasets...\n",
            "Collected 1898 prefix/suffix pairs\n",
            "Starting parallel evaluation with 8 workers...\n",
            "Note: Model will be pickled to each worker (one-time overhead per worker)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee30407c7e684797ba6616e82f684b06",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating robustness:   0%|          | 0/1898 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Process ForkProcess-6:\n",
            "Process ForkProcess-8:\n",
            "Process ForkProcess-7:\n",
            "Process ForkProcess-2:\n",
            "Process ForkProcess-3:\n",
            "Process ForkProcess-5:\n"
          ]
        },
        {
          "ename": "BrokenPipeError",
          "evalue": "[Errno 32] Broken pipe",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
            "File \u001b[0;32m/usr/lib64/python3.13/multiprocessing/resource_sharer.py:138\u001b[0m, in \u001b[0;36m_ResourceSharer._serve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 138\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_listener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccept\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[1;32m    139\u001b[0m             msg \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mrecv()\n\u001b[1;32m    140\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m msg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m/usr/lib64/python3.13/multiprocessing/connection.py:482\u001b[0m, in \u001b[0;36mListener.accept\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    480\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_listener\u001b[38;5;241m.\u001b[39maccept()\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_authkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 482\u001b[0m     \u001b[43mdeliver_challenge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_authkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    483\u001b[0m     answer_challenge(c, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_authkey)\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m c\n",
            "File \u001b[0;32m/usr/lib64/python3.13/multiprocessing/connection.py:938\u001b[0m, in \u001b[0;36mdeliver_challenge\u001b[0;34m(connection, authkey, digest_name)\u001b[0m\n\u001b[1;32m    934\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m}\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (digest_name\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mascii\u001b[39m\u001b[38;5;124m'\u001b[39m), message)\n\u001b[1;32m    935\u001b[0m \u001b[38;5;66;03m# Even when sending a challenge to a legacy client that does not support\u001b[39;00m\n\u001b[1;32m    936\u001b[0m \u001b[38;5;66;03m# digest prefixes, they'll take the entire thing as a challenge and\u001b[39;00m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;66;03m# respond to it with a raw HMAC-MD5.\u001b[39;00m\n\u001b[0;32m--> 938\u001b[0m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_CHALLENGE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    939\u001b[0m response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mrecv_bytes(\u001b[38;5;241m256\u001b[39m)        \u001b[38;5;66;03m# reject large message\u001b[39;00m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[0;32m/usr/lib64/python3.13/multiprocessing/connection.py:200\u001b[0m, in \u001b[0;36m_ConnectionBase.send_bytes\u001b[0;34m(self, buf, offset, size)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m offset \u001b[38;5;241m+\u001b[39m size \u001b[38;5;241m>\u001b[39m n:\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuffer length < offset + size\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m[\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m:\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/lib64/python3.13/multiprocessing/connection.py:427\u001b[0m, in \u001b[0;36mConnection._send_bytes\u001b[0;34m(self, buf)\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send(buf)\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;66;03m# Issue #20540: concatenate before sending, to avoid delays due\u001b[39;00m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;66;03m# to Nagle's algorithm on a TCP socket.\u001b[39;00m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;66;03m# Also note we want to avoid sending a 0-length buffer separately,\u001b[39;00m\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;66;03m# to avoid \"broken pipe\" errors if the other end closed the pipe.\u001b[39;00m\n\u001b[0;32m--> 427\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/lib64/python3.13/multiprocessing/connection.py:384\u001b[0m, in \u001b[0;36mConnection._send\u001b[0;34m(self, buf, write)\u001b[0m\n\u001b[1;32m    382\u001b[0m remaining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(buf)\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 384\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m     remaining \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m n\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Process ForkProcess-4:\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib64/python3.13/multiprocessing/process.py\", line 313, in _bootstrap\n",
            "    self.run()\n",
            "    ~~~~~~~~^^\n",
            "  File \"/usr/lib64/python3.13/multiprocessing/process.py\", line 313, in _bootstrap\n",
            "    self.run()\n",
            "    ~~~~~~~~^^\n",
            "  File \"/usr/lib64/python3.13/multiprocessing/process.py\", line 313, in _bootstrap\n",
            "    self.run()\n",
            "    ~~~~~~~~^^\n",
            "  File \"/usr/lib64/python3.13/multiprocessing/process.py\", line 313, in _bootstrap\n",
            "    self.run()\n",
            "    ~~~~~~~~^^\n",
            "  File \"/usr/lib64/python3.13/multiprocessing/process.py\", line 313, in _bootstrap\n",
            "    self.run()\n",
            "    ~~~~~~~~^^\n",
            "  File \"/usr/lib64/python3.13/multiprocessing/process.py\", line 313, in _bootstrap\n",
            "    self.run()\n",
            "    ~~~~~~~~^^\n",
            "  File \"/usr/lib64/python3.13/multiprocessing/process.py\", line 313, in _bootstrap\n",
            "    self.run()\n",
            "    ~~~~~~~~^^\n",
            "  File \"/usr/lib64/python3.13/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib64/python3.13/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib64/python3.13/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib64/python3.13/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib64/python3.13/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib64/python3.13/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib64/python3.13/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib64/python3.13/concurrent/futures/process.py\", line 242, in _process_worker\n",
            "    call_item = call_queue.get(block=True)\n",
            "  File \"/usr/lib64/python3.13/concurrent/futures/process.py\", line 242, in _process_worker\n",
            "    call_item = call_queue.get(block=True)\n",
            "  File \"/usr/lib64/python3.13/concurrent/futures/process.py\", line 242, in _process_worker\n",
            "    call_item = call_queue.get(block=True)\n",
            "  File \"/usr/lib64/python3.13/concurrent/futures/process.py\", line 242, in _process_worker\n",
            "    call_item = call_queue.get(block=True)\n",
            "  File \"/usr/lib64/python3.13/concurrent/futures/process.py\", line 242, in _process_worker\n",
            "    call_item = call_queue.get(block=True)\n",
            "  File \"/usr/lib64/python3.13/concurrent/futures/process.py\", line 242, in _process_worker\n",
            "    call_item = call_queue.get(block=True)\n",
            "  File \"/usr/lib64/python3.13/concurrent/futures/process.py\", line 242, in _process_worker\n",
            "    call_item = call_queue.get(block=True)\n",
            "  File \"/usr/lib64/python3.13/multiprocessing/queues.py\", line 100, in get\n",
            "    with self._rlock:\n",
            "         ^^^^^^^^^^^\n",
            "  File \"/usr/lib64/python3.13/multiprocessing/queues.py\", line 100, in get\n",
            "    with self._rlock:\n",
            "         ^^^^^^^^^^^\n",
            "  File \"/usr/lib64/python3.13/multiprocessing/queues.py\", line 100, in get\n",
            "    with self._rlock:\n",
            "         ^^^^^^^^^^^\n",
            "  File \"/usr/lib64/python3.13/multiprocessing/queues.py\", line 100, in get\n",
            "    with self._rlock:\n",
            "         ^^^^^^^^^^^\n",
            "  File \"/usr/lib64/python3.13/multiprocessing/queues.py\", line 100, in get\n",
            "    with self._rlock:\n",
            "         ^^^^^^^^^^^\n",
            "  File \"/usr/lib64/python3.13/multiprocessing/queues.py\", line 100, in get\n",
            "    with self._rlock:\n",
            "         ^^^^^^^^^^^\n",
            "  File \"/usr/lib64/python3.13/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "           ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/usr/lib64/python3.13/multiprocessing/queues.py\", line 101, in get\n",
            "    res = self._recv_bytes()\n",
            "  File \"/usr/lib64/python3.13/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "           ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/usr/lib64/python3.13/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "           ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/usr/lib64/python3.13/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "           ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/usr/lib64/python3.13/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "           ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
            "  File \"/usr/lib64/python3.13/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "           ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib64/python3.13/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib64/python3.13/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib64/python3.13/multiprocessing/connection.py\", line 395, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "KeyboardInterrupt\n",
            "Process ForkProcess-1:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib64/python3.13/multiprocessing/process.py\", line 313, in _bootstrap\n",
            "    self.run()\n",
            "    ~~~~~~~~^^\n",
            "  File \"/usr/lib64/python3.13/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib64/python3.13/concurrent/futures/process.py\", line 242, in _process_worker\n",
            "    call_item = call_queue.get(block=True)\n",
            "  File \"/usr/lib64/python3.13/multiprocessing/queues.py\", line 120, in get\n",
            "    return _ForkingPickler.loads(res)\n",
            "           ~~~~~~~~~~~~~~~~~~~~~^^^^^\n",
            "  File \"/home/chair/henryks_students/leon_urny/Robustness-in-suffix-prediction/.venv/lib64/python3.13/site-packages/torch/multiprocessing/reductions.py\", line 541, in rebuild_storage_fd\n",
            "    fd = df.detach()\n",
            "  File \"/usr/lib64/python3.13/multiprocessing/resource_sharer.py\", line 57, in detach\n",
            "    with _resource_sharer.get_connection(self._id) as conn:\n",
            "         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^\n",
            "  File \"/usr/lib64/python3.13/multiprocessing/resource_sharer.py\", line 86, in get_connection\n",
            "    c = Client(address, authkey=process.current_process().authkey)\n",
            "  File \"/usr/lib64/python3.13/multiprocessing/connection.py\", line 525, in Client\n",
            "    answer_challenge(c, authkey)\n",
            "    ~~~~~~~~~~~~~~~~^^^^^^^^^^^^\n",
            "  File \"/usr/lib64/python3.13/multiprocessing/connection.py\", line 953, in answer_challenge\n",
            "    message = connection.recv_bytes(256)         # reject large message\n",
            "  File \"/usr/lib64/python3.13/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "  File \"/usr/lib64/python3.13/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib64/python3.13/multiprocessing/connection.py\", line 395, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 117\u001b[0m\n\u001b[1;32m    114\u001b[0m     futures\u001b[38;5;241m.\u001b[39mappend(future)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# Process results as they complete\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconcurrent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_completed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluating robustness\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/henryks_students/leon_urny/Robustness-in-suffix-prediction/.venv/lib64/python3.13/site-packages/tqdm/notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 250\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# return super(tqdm...) will not catch exception\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
            "File \u001b[0;32m~/henryks_students/leon_urny/Robustness-in-suffix-prediction/.venv/lib64/python3.13/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
            "File \u001b[0;32m/usr/lib64/python3.13/concurrent/futures/_base.py:243\u001b[0m, in \u001b[0;36mas_completed\u001b[0;34m(fs, timeout)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wait_timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\n\u001b[1;32m    240\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m (of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) futures unfinished\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    241\u001b[0m                 \u001b[38;5;28mlen\u001b[39m(pending), total_futures))\n\u001b[0;32m--> 243\u001b[0m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m waiter\u001b[38;5;241m.\u001b[39mlock:\n\u001b[1;32m    246\u001b[0m     finished \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39mfinished_futures\n",
            "File \u001b[0;32m/usr/lib64/python3.13/threading.py:659\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    657\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 659\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
            "File \u001b[0;32m/usr/lib64/python3.13/threading.py:359\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 359\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Main evaluation loop with parallelization\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "save_every = 50\n",
        "results = {}\n",
        "concept_name = 'Activity'  # Match the concept_name used in ProbabilisticEvaluation\n",
        "num_workers = 8  # Number of parallel workers\n",
        "\n",
        "# Collect prefix/suffix pairs from predefined datasets (fast - just reading data)\n",
        "print(\"Collecting prefix/suffix pairs from predefined datasets...\")\n",
        "all_prefix_suffix_pairs_orig = list(redo_activity_dataset.items())\n",
        "all_prefix_suffix_pairs_pert = list(redo_activity_pert_dataset.items())\n",
        "\n",
        "# Ensure they're in the same order and match\n",
        "assert len(all_prefix_suffix_pairs_orig) == len(all_prefix_suffix_pairs_pert), \\\n",
        "    f\"Mismatch in number of pairs: {len(all_prefix_suffix_pairs_orig)} vs {len(all_prefix_suffix_pairs_pert)}\"\n",
        "\n",
        "# Verify keys match\n",
        "for (key_orig, _), (key_pert, _) in zip(all_prefix_suffix_pairs_orig, all_prefix_suffix_pairs_pert):\n",
        "    assert key_orig == key_pert, f\"Key mismatch: {key_orig} != {key_pert}\"\n",
        "\n",
        "print(f\"Collected {len(all_prefix_suffix_pairs_orig)} prefix/suffix pairs\")\n",
        "\n",
        "# Helper function to evaluate and process a single pair (runs in worker process)\n",
        "def evaluate_and_process_pair(\n",
        "    eval_orig_instance, eval_pert_instance,\n",
        "    case_name, prefix_len, prefix_orig, suffix_orig, prefix_pert, suffix_pert,\n",
        "    filter_prediction_events, calculate_remaining_time, \n",
        "    calculate_sampled_remaining_times, concept_name\n",
        "):\n",
        "    \"\"\"Evaluate a single pair and process results - runs in worker process\"\"\"\n",
        "    # Evaluate both (this is the expensive model inference part)\n",
        "    orig_result = eval_orig_instance._evaluate_single(case_name, prefix_len, prefix_orig, suffix_orig, False)\n",
        "    pert_result = eval_pert_instance._evaluate_single(case_name, prefix_len, prefix_pert, suffix_pert, False)\n",
        "    \n",
        "    (case_name_orig, prefix_len_orig, prefix_orig_readable, predicted_suffixes_orig, suffix_orig_readable, mean_pred_orig) = orig_result\n",
        "    (case_name_pert, prefix_len_pert, prefix_pert_readable, predicted_suffixes_pert, suffix_pert_readable, mean_pred_pert) = pert_result\n",
        "    \n",
        "    # Ensure we're comparing the same case and prefix length\n",
        "    assert case_name_orig == case_name_pert, f\"Case mismatch: {case_name_orig} != {case_name_pert}\"\n",
        "    assert prefix_len_orig == prefix_len_pert, f\"Prefix length mismatch: {prefix_len_orig} != {prefix_len_pert}\"\n",
        "    \n",
        "    # Filter predictions to only include concept:name and case_elapsed_time\n",
        "    mean_pred_orig_filtered = filter_prediction_events(mean_pred_orig, concept_name=concept_name)\n",
        "    predicted_suffixes_orig_filtered = [filter_prediction_events(sample, concept_name=concept_name) \n",
        "                                        for sample in predicted_suffixes_orig] if predicted_suffixes_orig else None\n",
        "    \n",
        "    mean_pred_pert_filtered = filter_prediction_events(mean_pred_pert, concept_name=concept_name)\n",
        "    predicted_suffixes_pert_filtered = [filter_prediction_events(sample, concept_name=concept_name) \n",
        "                                        for sample in predicted_suffixes_pert] if predicted_suffixes_pert else None\n",
        "    \n",
        "    # Calculate remaining times\n",
        "    mean_pred_remaining_time_orig = calculate_remaining_time(prefix_orig_readable, mean_pred_orig, concept_name=concept_name)\n",
        "    sampled_remaining_time_orig = calculate_sampled_remaining_times(prefix_orig_readable, predicted_suffixes_orig, concept_name=concept_name)\n",
        "    \n",
        "    mean_pred_remaining_time_pert = calculate_remaining_time(prefix_pert_readable, mean_pred_pert, concept_name=concept_name)\n",
        "    sampled_remaining_time_pert = calculate_sampled_remaining_times(prefix_pert_readable, predicted_suffixes_pert, concept_name=concept_name)\n",
        "\n",
        "    # Return structured result\n",
        "    key = (case_name_orig, prefix_len_orig)\n",
        "    return key, {\n",
        "        'original': (\n",
        "            prefix_orig_readable,  # Keep all fields\n",
        "            suffix_orig_readable,  # Keep all fields\n",
        "            mean_pred_orig_filtered,  # Filtered: only concept:name and case_elapsed_time\n",
        "            predicted_suffixes_orig_filtered,  # Filtered: only concept:name and case_elapsed_time\n",
        "            mean_pred_remaining_time_orig,  # NEW: single float\n",
        "            sampled_remaining_time_orig  # NEW: list of floats\n",
        "        ),\n",
        "        'perturbed': (\n",
        "            prefix_pert_readable,  # Keep all fields\n",
        "            suffix_pert_readable,  # Keep all fields\n",
        "            mean_pred_pert_filtered,  # Filtered: only concept:name and case_elapsed_time\n",
        "            predicted_suffixes_pert_filtered,  # Filtered: only concept:name and case_elapsed_time\n",
        "            mean_pred_remaining_time_pert,  # NEW: single float\n",
        "            sampled_remaining_time_pert  # NEW: list of floats\n",
        "        ),\n",
        "    }\n",
        "\n",
        "# Process in parallel\n",
        "processed = 0\n",
        "successful = 0\n",
        "failed = []\n",
        "\n",
        "print(f\"Starting parallel evaluation with {num_workers} workers...\")\n",
        "print(\"Note: Model will be pickled to each worker (one-time overhead per worker)\")\n",
        "\n",
        "# Create dictionary for O(1) lookup of perturbed pairs\n",
        "perturbed_dict = dict(all_prefix_suffix_pairs_pert)\n",
        "\n",
        "with concurrent.futures.ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
        "    # Submit all tasks - each worker gets a prefix/suffix pair\n",
        "    futures = []\n",
        "    for (case_name, prefix_len), (prefix_orig, suffix_orig) in all_prefix_suffix_pairs_orig:\n",
        "        # Get corresponding perturbed pair using dictionary lookup (O(1) instead of O(n))\n",
        "        (prefix_pert, suffix_pert) = perturbed_dict[(case_name, prefix_len)]\n",
        "        \n",
        "        # Clone tensors to avoid memory issues (as done in evaluate_multi_processing)\n",
        "        prefix_orig_clone = [[t.clone() for t in i] for i in prefix_orig] if isinstance(prefix_orig[0], list) else prefix_orig\n",
        "        suffix_orig_clone = [[t.clone() for t in i] for i in suffix_orig] if isinstance(suffix_orig[0], list) else suffix_orig\n",
        "        prefix_pert_clone = [[t.clone() for t in i] for i in prefix_pert] if isinstance(prefix_pert[0], list) else prefix_pert\n",
        "        suffix_pert_clone = [[t.clone() for t in i] for i in suffix_pert] if isinstance(suffix_pert[0], list) else suffix_pert\n",
        "        \n",
        "        # Submit task\n",
        "        future = executor.submit(\n",
        "            evaluate_and_process_pair,\n",
        "            eval_original, eval_perturbed,\n",
        "            case_name, prefix_len,\n",
        "            prefix_orig_clone, suffix_orig_clone,\n",
        "            prefix_pert_clone, suffix_pert_clone,\n",
        "            filter_prediction_events, calculate_remaining_time,\n",
        "            calculate_sampled_remaining_times, concept_name\n",
        "        )\n",
        "        futures.append(future)\n",
        "    \n",
        "    # Process results as they complete\n",
        "    for future in tqdm(concurrent.futures.as_completed(futures), \n",
        "                       total=len(futures), desc=\"Evaluating robustness\"):\n",
        "        try:\n",
        "            key, result = future.result()\n",
        "            results[key] = result\n",
        "            processed += 1\n",
        "            successful += 1\n",
        "            \n",
        "            # Save periodically\n",
        "            if processed % save_every == 0:\n",
        "                save_chunk(results, processed - 1, output_dir)\n",
        "                results = {}\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"Error processing pair: {e}\")\n",
        "            failed.append(str(e))\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "# Save remaining results\n",
        "if len(results):\n",
        "    save_chunk(results, processed - 1, output_dir)\n",
        "\n",
        "print(f\"\\nRobustness evaluation completed!\")\n",
        "print(f\"  Processed: {successful}/{len(all_prefix_suffix_pairs_orig)} pairs successfully\")\n",
        "if failed:\n",
        "    print(f\"  Failed: {len(failed)} pairs\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'os' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m all_results \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Get all chunk files and sort them\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m chunk_files \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mlistdir(output_dir) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrobustness_results_part_\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m      5\u001b[0m chunk_files\u001b[38;5;241m.\u001b[39msort()  \u001b[38;5;66;03m# Ensure correct order\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunk_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m chunk files\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ],
      "source": [
        "# Load all saved chunks and combine them\n",
        "all_results = {}\n",
        "# Get all chunk files and sort them\n",
        "chunk_files = [f for f in os.listdir(output_dir) if f.startswith('robustness_results_part_')]\n",
        "chunk_files.sort()  # Ensure correct order\n",
        "\n",
        "print(f\"Found {len(chunk_files)} chunk files\")\n",
        "\n",
        "for chunk_file in chunk_files:\n",
        "    chunk_path = os.path.join(output_dir, chunk_file)\n",
        "    print(f\"Loading {chunk_file}...\")\n",
        "    with open(chunk_path, 'rb') as f:\n",
        "        chunk_results = pickle.load(f)\n",
        "        all_results.update(chunk_results)\n",
        "        print(f\"  Added {len(chunk_results)} results from {chunk_file}\")\n",
        "\n",
        "# Also add the final results if any (e.g. from a still-running evaluation loop)\n",
        "if 'results' in locals() and len(results) > 0:\n",
        "    print(f\"Adding final {len(results)} results...\")\n",
        "    all_results.update(results)\n",
        "\n",
        "print(f\"\\nTotal results loaded: {len(all_results)}\")\n",
        "\n",
        "# Save combined results into a single pickle file\n",
        "combined_results_path = os.path.join(output_dir, 'robustness_results.pkl')\n",
        "with open(combined_results_path, 'wb') as f:\n",
        "    pickle.dump(all_results, f)\n",
        "\n",
        "print(f\"Combined results saved to {combined_results_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
