{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import importlib\n",
        "import sys\n",
        "import torch\n",
        "import pickle\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "sys.path.insert(0, '..')\n",
        "sys.path.insert(0, '../..')\n",
        "sys.path.insert(0, '../../..')\n",
        "sys.path.insert(0, '../../../..')\n",
        "sys.path.insert(0, '../../../../..')\n",
        "\n",
        "from model.dropout_uncertainty_enc_dec_LSTM.dropout_uncertainty_model import DropoutUncertaintyEncoderDecoderLSTM\n",
        "from evaluation.probabilistic_evaluation import ProbabilisticEvaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data set categories:  ([('concept:name', 18, {'Admission IC': 1, 'Admission NC': 2, 'CRP': 3, 'EOS': 4, 'ER Registration': 5, 'ER Sepsis Triage': 6, 'ER Triage': 7, 'IV Antibiotics': 8, 'IV Liquid': 9, 'LacticAcid': 10, 'Leucocytes': 11, 'Release A': 12, 'Release B': 13, 'Release C': 14, 'Release D': 15, 'Release E': 16, 'Return ER': 17}), ('InfectionSuspected', 5, {'EOS': 1, 'False': 2, 'True': 3, nan: 4}), ('org:group', 27, {'?': 1, 'A': 2, 'B': 3, 'C': 4, 'D': 5, 'E': 6, 'EOS': 7, 'F': 8, 'G': 9, 'H': 10, 'I': 11, 'J': 12, 'K': 13, 'L': 14, 'M': 15, 'N': 16, 'O': 17, 'P': 18, 'Q': 19, 'R': 20, 'S': 21, 'T': 22, 'U': 23, 'V': 24, 'W': 25, 'Y': 26}), ('DiagnosticBlood', 5, {'EOS': 1, 'False': 2, 'True': 3, nan: 4}), ('DisfuncOrg', 5, {'EOS': 1, 'False': 2, 'True': 3, nan: 4}), ('SIRSCritTachypnea', 5, {'EOS': 1, 'False': 2, 'True': 3, nan: 4}), ('Hypotensie', 5, {'EOS': 1, 'False': 2, 'True': 3, nan: 4}), ('SIRSCritHeartRate', 5, {'EOS': 1, 'False': 2, 'True': 3, nan: 4}), ('Infusion', 5, {'EOS': 1, 'False': 2, 'True': 3, nan: 4}), ('DiagnosticArtAstrup', 5, {'EOS': 1, 'False': 2, 'True': 3, nan: 4}), ('DiagnosticIC', 5, {'EOS': 1, 'False': 2, 'True': 3, nan: 4}), ('DiagnosticSputum', 5, {'EOS': 1, 'False': 2, 'True': 3, nan: 4}), ('DiagnosticLiquor', 5, {'EOS': 1, 'False': 2, 'True': 3, nan: 4}), ('DiagnosticOther', 5, {'EOS': 1, 'False': 2, 'True': 3, nan: 4}), ('SIRSCriteria2OrMore', 5, {'EOS': 1, 'False': 2, 'True': 3, nan: 4}), ('DiagnosticXthorax', 5, {'EOS': 1, 'False': 2, 'True': 3, nan: 4}), ('SIRSCritTemperature', 5, {'EOS': 1, 'False': 2, 'True': 3, nan: 4}), ('DiagnosticUrinaryCulture', 5, {'EOS': 1, 'False': 2, 'True': 3, nan: 4}), ('SIRSCritLeucos', 5, {'EOS': 1, 'False': 2, 'True': 3, nan: 4}), ('Oligurie', 5, {'EOS': 1, 'False': 2, 'True': 3, nan: 4}), ('DiagnosticLacticAcid', 5, {'EOS': 1, 'False': 2, 'True': 3, nan: 4}), ('lifecycle:transition', 3, {'EOS': 1, 'complete': 2}), ('Diagnose', 117, {'AA': 1, 'AC': 2, 'AD': 3, 'AE': 4, 'B': 5, 'BB': 6, 'BC': 7, 'BD': 8, 'C': 9, 'CA': 10, 'CB': 11, 'CC': 12, 'CD': 13, 'D': 14, 'DB': 15, 'DD': 16, 'DE': 17, 'E': 18, 'EA': 19, 'EC': 20, 'ED': 21, 'EE': 22, 'EOS': 23, 'F': 24, 'FA': 25, 'FC': 26, 'FD': 27, 'FE': 28, 'G': 29, 'GA': 30, 'GB': 31, 'GC': 32, 'GD': 33, 'GE': 34, 'H': 35, 'HA': 36, 'HB': 37, 'HC': 38, 'HD': 39, 'HE': 40, 'I': 41, 'IA': 42, 'IC': 43, 'J': 44, 'JC': 45, 'JE': 46, 'K': 47, 'KA': 48, 'KB': 49, 'KC': 50, 'KD': 51, 'L': 52, 'LA': 53, 'LB': 54, 'LC': 55, 'LD': 56, 'LE': 57, 'M': 58, 'MA': 59, 'MB': 60, 'MC': 61, 'MD': 62, 'ME': 63, 'N': 64, 'NB': 65, 'NC': 66, 'ND': 67, 'O': 68, 'OB': 69, 'OC': 70, 'OD': 71, 'OE': 72, 'P': 73, 'PC': 74, 'PD': 75, 'Q': 76, 'QB': 77, 'QD': 78, 'QE': 79, 'R': 80, 'RA': 81, 'RB': 82, 'RC': 83, 'RD': 84, 'S': 85, 'SA': 86, 'SB': 87, 'SD': 88, 'T': 89, 'TB': 90, 'U': 91, 'UA': 92, 'UC': 93, 'UD': 94, 'V': 95, 'VA': 96, 'VB': 97, 'VD': 98, 'W': 99, 'WA': 100, 'WB': 101, 'WC': 102, 'WD': 103, 'X': 104, 'XA': 105, 'XB': 106, 'XC': 107, 'XD': 108, 'Y': 109, 'YA': 110, 'YC': 111, 'Z': 112, 'ZA': 113, 'ZB': 114, 'ZD': 115, nan: 116}), ('Hypoxie', 5, {'EOS': 1, 'False': 2, 'True': 3, nan: 4}), ('DiagnosticUrinarySediment', 5, {'EOS': 1, 'False': 2, 'True': 3, nan: 4}), ('DiagnosticECG', 5, {'EOS': 1, 'False': 2, 'True': 3, nan: 4})], [('case_elapsed_time', 1, {}), ('event_elapsed_time', 1, {}), ('day_in_week', 1, {}), ('seconds_in_day', 1, {}), ('Age', 1, {}), ('Leucocytes', 1, {}), ('CRP', 1, {}), ('LacticAcid', 1, {})])\n",
            "Encoder input features:  [['concept:name', 'InfectionSuspected', 'org:group', 'DiagnosticBlood', 'DisfuncOrg', 'SIRSCritTachypnea', 'Hypotensie', 'SIRSCritHeartRate', 'Infusion', 'DiagnosticArtAstrup', 'DiagnosticIC', 'DiagnosticSputum', 'DiagnosticLiquor', 'DiagnosticOther', 'SIRSCriteria2OrMore', 'DiagnosticXthorax', 'SIRSCritTemperature', 'DiagnosticUrinaryCulture', 'SIRSCritLeucos', 'Oligurie', 'DiagnosticLacticAcid', 'lifecycle:transition', 'Diagnose', 'Hypoxie', 'DiagnosticUrinarySediment', 'DiagnosticECG'], ['case_elapsed_time', 'event_elapsed_time', 'day_in_week', 'seconds_in_day', 'Age', 'Leucocytes', 'CRP', 'LacticAcid']]\n",
            "Decoder input+output features:  [['concept:name', 'org:group', 'lifecycle:transition'], ['case_elapsed_time', 'event_elapsed_time']]\n",
            "\n",
            "\n",
            "Sequence length of decoder output:  4\n",
            "\n",
            "\n",
            "Cells hidden size:  128\n",
            "Number of LSTM layer:  4\n",
            "Dropout rate:  0.1\n",
            "\n",
            "\n",
            "Encoder number of labels for each input feature (categorical, numerical):  [[18, 5, 27, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 117, 5, 5, 5], [1, 1, 1, 1, 1, 1, 1, 1]]\n",
            "Encoder indices of tensors in dataset used as input:  [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [0, 1, 2, 3, 4, 5, 6, 7]]\n",
            "Embeddings encoder:  ModuleList(\n",
            "  (0): Embedding(18, 8)\n",
            "  (1): Embedding(5, 4)\n",
            "  (2): Embedding(27, 10)\n",
            "  (3-20): 18 x Embedding(5, 4)\n",
            "  (21): Embedding(3, 3)\n",
            "  (22): Embedding(117, 23)\n",
            "  (23-25): 3 x Embedding(5, 4)\n",
            ")\n",
            "Total embedding feature size encoder:  132\n",
            "Total numerical feature size encoder:  8\n",
            "Input feature size encoder:  140\n",
            "Encoder initialized! \n",
            "\n",
            "Decoder label values size for each categorical input feature:  [18, 27, 3]\n",
            "Decoder label values size for each numerical input feature:  [1, 1]\n",
            "Decoder indices of tensors in dataset used as input:  [[0, 2, 21], [0, 1]]\n",
            "Embeddings decoder:  ModuleList(\n",
            "  (0): Embedding(18, 8)\n",
            "  (1): Embedding(27, 10)\n",
            "  (2): Embedding(3, 3)\n",
            ")\n",
            "Total embedding feature size decoder:  21\n",
            "Total numerical feature size decoder:  2\n",
            "Input feature size decoder:  23\n",
            "Output feature list of dicts (featue name, feature output size) of decoder:  [{'concept:name': 18, 'org:group': 27, 'lifecycle:transition': 3}, {'case_elapsed_time': 1, 'event_elapsed_time': 1}]\n",
            "Decoder initialized! \n",
            "\n",
            "Output feature list of dicts (featue name, tensor index in dataset) of decoder:  [{'concept:name': 0, 'org:group': 2, 'lifecycle:transition': 21}, {'case_elapsed_time': 0, 'event_elapsed_time': 1}]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/chair/henryks_students/leon_urny/Robustness-in-suffix-prediction/.venv/lib64/python3.13/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator OrdinalEncoder from version 1.7.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/home/chair/henryks_students/leon_urny/Robustness-in-suffix-prediction/.venv/lib64/python3.13/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator SimpleImputer from version 1.7.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/home/chair/henryks_students/leon_urny/Robustness-in-suffix-prediction/.venv/lib64/python3.13/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.7.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/home/chair/henryks_students/leon_urny/Robustness-in-suffix-prediction/.venv/lib64/python3.13/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator OrdinalEncoder from version 1.7.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/home/chair/henryks_students/leon_urny/Robustness-in-suffix-prediction/.venv/lib64/python3.13/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator SimpleImputer from version 1.7.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/home/chair/henryks_students/leon_urny/Robustness-in-suffix-prediction/.venv/lib64/python3.13/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.7.1 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original dataset loaded: 3237 cases\n",
            "Perturbed dataset loaded: 3237 cases\n"
          ]
        }
      ],
      "source": [
        "# Load model\n",
        "#file_path_model = '../../../training_variational_dropout/sepsis/Sepsis_full_no_grad_norm_robustness.pkl'\n",
        "file_path_model = '../../../training_variational_dropout/Sepsis/Sepsis_setting_2.pkl'\n",
        "output_dir = '../../../../../evaluation_results/robustness/sepsis/redo_activity/'\n",
        "model = DropoutUncertaintyEncoderDecoderLSTM.load(file_path_model, dropout=0.1)\n",
        "\n",
        "# Load datasets\n",
        "file_path_original = '../../../../../encoded_data/sepsis/Sepsis_all_5_test.pkl'\n",
        "file_path_perturbed = '../../../../../encoded_data/sepsis/Sepsis_all_5_test.pkl'\n",
        "file_path_redo_activity = '../../../../../encoded_data/sepsis/val.pkl'\n",
        "file_path_redo_activity_pert = '../../../../../encoded_data/sepsis/redo_activity.pkl'\n",
        "\n",
        "\n",
        "original_dataset = torch.load(file_path_original, weights_only=False)\n",
        "perturbed_dataset = torch.load(file_path_perturbed, weights_only=False)\n",
        "redo_activity_dataset = torch.load(file_path_redo_activity, weights_only=False)\n",
        "redo_activity_pert_dataset = torch.load(file_path_redo_activity_pert, weights_only=False)\n",
        "\n",
        "\n",
        "print(f\"Original dataset loaded: {len(original_dataset)} cases\")\n",
        "print(f\"Perturbed dataset loaded: {len(perturbed_dataset)} cases\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ProbabilisticEvaluation instances created\n"
          ]
        }
      ],
      "source": [
        "# Create evaluation instances (NON-RANDOM ORDER)\n",
        "eval_original = ProbabilisticEvaluation(\n",
        "    model, original_dataset,\n",
        "    concept_name='concept:name',\n",
        "    num_processes=16, \n",
        "    growing_num_values=['case_elapsed_time'],\n",
        "    samples_per_case=100,\n",
        "    sample_argmax=False,\n",
        "    use_variance_cat=True,\n",
        "    use_variance_num=True,\n",
        "    all_cat=['concept:name', 'org:group', 'lifecycle:transition'],\n",
        "    all_num=['case_elapsed_time', 'event_elapsed_time'],\n",
        "    dataset_predefined_prefixes=redo_activity_dataset\n",
        ")\n",
        "\n",
        "eval_perturbed = ProbabilisticEvaluation(\n",
        "    model, perturbed_dataset,\n",
        "    concept_name='concept:name', #'Activity'\n",
        "    num_processes=16,\n",
        "    growing_num_values=['case_elapsed_time'],\n",
        "    samples_per_case=100,\n",
        "    sample_argmax=False,\n",
        "    use_variance_cat=True,\n",
        "    use_variance_num=True,\n",
        "    all_cat=['concept:name', 'org:group', 'lifecycle:transition'],\n",
        "    all_num=['case_elapsed_time', 'event_elapsed_time'],\n",
        "    dataset_predefined_prefixes=redo_activity_pert_dataset\n",
        ")\n",
        "\n",
        "print(\"ProbabilisticEvaluation instances created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Robustness metrics module imported\n"
          ]
        }
      ],
      "source": [
        "# Import robustness metrics module\n",
        "import robustness.evaluator.robustness_metrics\n",
        "#importlib.reload(robustness.robustness_metrics)\n",
        "from robustness.evaluator.robustness_metrics import save_chunk\n",
        "\n",
        "print(\"Robustness metrics module imported\")\n",
        "\n",
        "# Helper functions for filtering predictions and calculating remaining time\n",
        "def filter_prediction_events(prediction_list, concept_name='concept:name'):\n",
        "    \"\"\"Filter prediction events to only include concept:name and case_elapsed_time\"\"\"\n",
        "    if prediction_list is None:\n",
        "        return None\n",
        "    filtered = []\n",
        "    for event in prediction_list:\n",
        "        if not isinstance(event, dict):\n",
        "            continue\n",
        "        filtered_event = {}\n",
        "        if concept_name in event:\n",
        "            filtered_event[concept_name] = event[concept_name]\n",
        "        if 'case_elapsed_time' in event:\n",
        "            filtered_event['case_elapsed_time'] = event['case_elapsed_time']\n",
        "        filtered.append(filtered_event)\n",
        "    return filtered\n",
        "\n",
        "def calculate_remaining_time(prefix, prediction, concept_name='concept:name'):\n",
        "    \"\"\"Calculate remaining time from prefix and prediction\"\"\"\n",
        "    if not prefix or not prediction:\n",
        "        return None\n",
        "    if not isinstance(prefix[-1], dict) or 'case_elapsed_time' not in prefix[-1]:\n",
        "        return None\n",
        "    if not isinstance(prediction[-1], dict) or 'case_elapsed_time' not in prediction[-1]:\n",
        "        return None\n",
        "    current_time = prefix[-1]['case_elapsed_time']\n",
        "    final_time = prediction[-1]['case_elapsed_time']\n",
        "    return final_time - current_time\n",
        "\n",
        "def calculate_sampled_remaining_times(prefix, predicted_suffixes, concept_name='concept:name'):\n",
        "    \"\"\"Calculate remaining time for each sample in predicted_suffixes\"\"\"\n",
        "    if not prefix or not predicted_suffixes:\n",
        "        return None\n",
        "    if not isinstance(prefix[-1], dict) or 'case_elapsed_time' not in prefix[-1]:\n",
        "        return None\n",
        "    current_time = prefix[-1]['case_elapsed_time']\n",
        "    \n",
        "    remaining_times = []\n",
        "    for sample in predicted_suffixes:\n",
        "        if not sample or not isinstance(sample[-1], dict) or 'case_elapsed_time' not in sample[-1]:\n",
        "            remaining_times.append(None)\n",
        "        else:\n",
        "            final_time = sample[-1]['case_elapsed_time']\n",
        "            remaining_times.append(final_time - current_time)\n",
        "    return remaining_times\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "44aa9fc98d574a94a888dd1be52a61b5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating robustness: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0b12337b79ef46cb87d291f0f2cb03ac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1754 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "96b4fa6b1c554e44b69bee12a0800112",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1754 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 50 results to ../../../../../evaluation_results/robustness/sepsis/redo_activity/robustness_results_part_050.pkl\n",
            "Saved 50 results to ../../../../../evaluation_results/robustness/sepsis/redo_activity/robustness_results_part_100.pkl\n",
            "Saved 50 results to ../../../../../evaluation_results/robustness/sepsis/redo_activity/robustness_results_part_150.pkl\n",
            "Saved 50 results to ../../../../../evaluation_results/robustness/sepsis/redo_activity/robustness_results_part_200.pkl\n",
            "Saved 50 results to ../../../../../evaluation_results/robustness/sepsis/redo_activity/robustness_results_part_250.pkl\n",
            "Saved 50 results to ../../../../../evaluation_results/robustness/sepsis/redo_activity/robustness_results_part_300.pkl\n",
            "Saved 50 results to ../../../../../evaluation_results/robustness/sepsis/redo_activity/robustness_results_part_350.pkl\n",
            "Saved 50 results to ../../../../../evaluation_results/robustness/sepsis/redo_activity/robustness_results_part_400.pkl\n",
            "Saved 50 results to ../../../../../evaluation_results/robustness/sepsis/redo_activity/robustness_results_part_450.pkl\n"
          ]
        }
      ],
      "source": [
        "# Main evaluation loop\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "save_every = 50\n",
        "results = {}\n",
        "concept_name = 'concept:name'  # Match the concept_name used in ProbabilisticEvaluation\n",
        "\n",
        "for i, ((case_name_orig, prefix_len_orig, prefix_orig, predicted_suffixes_orig, suffix_orig, mean_pred_orig),\n",
        "        (case_name_pert, prefix_len_pert, prefix_pert, predicted_suffixes_pert, suffix_pert, mean_pred_pert)) in enumerate(\n",
        "    tqdm(zip(eval_original.evaluate_with_predifined_prefix(random_order=False), \n",
        "             eval_perturbed.evaluate_with_predifined_prefix(random_order=False)), \n",
        "         desc=\"Evaluating robustness\")):\n",
        "    \n",
        "    # Ensure we're comparing the same case and prefix length\n",
        "    assert case_name_orig == case_name_pert, f\"Case mismatch: {case_name_orig} != {case_name_pert}\"\n",
        "\n",
        "    assert prefix_len_orig == prefix_len_pert, f\"Prefix length mismatch: {prefix_len_orig} != {prefix_len_pert}\"\n",
        "\n",
        "    # Filter predictions to only include concept:name and case_elapsed_time\n",
        "    mean_pred_orig_filtered = filter_prediction_events(mean_pred_orig, concept_name=concept_name)\n",
        "    predicted_suffixes_orig_filtered = [filter_prediction_events(sample, concept_name=concept_name) \n",
        "                                        for sample in predicted_suffixes_orig] if predicted_suffixes_orig else None\n",
        "    \n",
        "    mean_pred_pert_filtered = filter_prediction_events(mean_pred_pert, concept_name=concept_name)\n",
        "    predicted_suffixes_pert_filtered = [filter_prediction_events(sample, concept_name=concept_name) \n",
        "                                        for sample in predicted_suffixes_pert] if predicted_suffixes_pert else None\n",
        "    \n",
        "    # Calculate remaining times immediately\n",
        "    mean_pred_remaining_time_orig = calculate_remaining_time(prefix_orig, mean_pred_orig, concept_name=concept_name)\n",
        "    sampled_remaining_time_orig = calculate_sampled_remaining_times(prefix_orig, predicted_suffixes_orig, concept_name=concept_name)\n",
        "    \n",
        "    mean_pred_remaining_time_pert = calculate_remaining_time(prefix_pert, mean_pred_pert, concept_name=concept_name)\n",
        "    sampled_remaining_time_pert = calculate_sampled_remaining_times(prefix_pert, predicted_suffixes_pert, concept_name=concept_name)\n",
        "\n",
        "    # Store results with new structure\n",
        "    key = (case_name_orig, prefix_len_orig)\n",
        "    results[key] = {\n",
        "        'original': (\n",
        "            prefix_orig,  # Keep all fields\n",
        "            suffix_orig,  # Keep all fields\n",
        "            mean_pred_orig_filtered,  # Filtered: only concept:name and case_elapsed_time\n",
        "            predicted_suffixes_orig_filtered,  # Filtered: only concept:name and case_elapsed_time\n",
        "            mean_pred_remaining_time_orig,  # NEW: single float\n",
        "            sampled_remaining_time_orig  # NEW: list of floats\n",
        "        ),\n",
        "        'perturbed': (\n",
        "            prefix_pert,  # Keep all fields\n",
        "            suffix_pert,  # Keep all fields\n",
        "            mean_pred_pert_filtered,  # Filtered: only concept:name and case_elapsed_time\n",
        "            predicted_suffixes_pert_filtered,  # Filtered: only concept:name and case_elapsed_time\n",
        "            mean_pred_remaining_time_pert,  # NEW: single float\n",
        "            sampled_remaining_time_pert  # NEW: list of floats\n",
        "        ),\n",
        "    }\n",
        "\n",
        "    \n",
        "    if (i + 1) % save_every == 0:\n",
        "        save_chunk(results, i, output_dir)\n",
        "        results = {}\n",
        "\n",
        "if len(results):\n",
        "    save_chunk(results, i, output_dir)\n",
        "\n",
        "print(\"Robustness evaluation completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 10 chunk files\n",
            "Loading robustness_results_part_050.pkl...\n",
            "  Added 50 results from robustness_results_part_050.pkl\n",
            "Loading robustness_results_part_100.pkl...\n",
            "  Added 50 results from robustness_results_part_100.pkl\n",
            "Loading robustness_results_part_150.pkl...\n",
            "  Added 50 results from robustness_results_part_150.pkl\n",
            "Loading robustness_results_part_200.pkl...\n",
            "  Added 50 results from robustness_results_part_200.pkl\n",
            "Loading robustness_results_part_250.pkl...\n",
            "  Added 50 results from robustness_results_part_250.pkl\n",
            "Loading robustness_results_part_300.pkl...\n",
            "  Added 50 results from robustness_results_part_300.pkl\n",
            "Loading robustness_results_part_350.pkl...\n",
            "  Added 50 results from robustness_results_part_350.pkl\n",
            "Loading robustness_results_part_400.pkl...\n",
            "  Added 50 results from robustness_results_part_400.pkl\n",
            "Loading robustness_results_part_450.pkl...\n",
            "  Added 50 results from robustness_results_part_450.pkl\n",
            "Loading robustness_results_part_500.pkl...\n",
            "  Added 50 results from robustness_results_part_500.pkl\n",
            "\n",
            "Total results loaded: 500\n",
            "Combined results saved to ../../../../../evaluation_results/robustness/sepsis/redo_activity/robustness_results.pkl\n"
          ]
        }
      ],
      "source": [
        "# Load all saved chunks and combine them\n",
        "all_results = {}\n",
        "# Get all chunk files and sort them\n",
        "chunk_files = [f for f in os.listdir(output_dir) if f.startswith('robustness_results_part_')]\n",
        "chunk_files.sort()  # Ensure correct order\n",
        "\n",
        "print(f\"Found {len(chunk_files)} chunk files\")\n",
        "\n",
        "for chunk_file in chunk_files:\n",
        "    chunk_path = os.path.join(output_dir, chunk_file)\n",
        "    print(f\"Loading {chunk_file}...\")\n",
        "    with open(chunk_path, 'rb') as f:\n",
        "        chunk_results = pickle.load(f)\n",
        "        all_results.update(chunk_results)\n",
        "        print(f\"  Added {len(chunk_results)} results from {chunk_file}\")\n",
        "\n",
        "# Also add the final results if any (e.g. from a still-running evaluation loop)\n",
        "if 'results' in locals() and len(results) > 0:\n",
        "    print(f\"Adding final {len(results)} results...\")\n",
        "    all_results.update(results)\n",
        "\n",
        "print(f\"\\nTotal results loaded: {len(all_results)}\")\n",
        "\n",
        "# Save combined results into a single pickle file\n",
        "combined_results_path = os.path.join(output_dir, 'robustness_results.pkl')\n",
        "with open(combined_results_path, 'wb') as f:\n",
        "    pickle.dump(all_results, f)\n",
        "\n",
        "print(f\"Combined results saved to {combined_results_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
