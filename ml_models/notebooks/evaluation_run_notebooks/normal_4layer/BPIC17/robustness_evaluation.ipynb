{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import importlib\n",
        "import sys\n",
        "import torch\n",
        "import pickle\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "sys.path.insert(0, '..')\n",
        "sys.path.insert(0, '../..')\n",
        "sys.path.insert(0, '../../..')\n",
        "sys.path.insert(0, '../../../..')\n",
        "sys.path.insert(0, '../../../../..')\n",
        "\n",
        "from model.dropout_uncertainty_enc_dec_LSTM.dropout_uncertainty_model import DropoutUncertaintyEncoderDecoderLSTM\n",
        "from evaluation.probabilistic_evaluation import ProbabilisticEvaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data set categories:  ([('concept:name', 28, {'A_Accepted': 1, 'A_Cancelled': 2, 'A_Complete': 3, 'A_Concept': 4, 'A_Create Application': 5, 'A_Denied': 6, 'A_Incomplete': 7, 'A_Pending': 8, 'A_Submitted': 9, 'A_Validating': 10, 'EOS': 11, 'O_Accepted': 12, 'O_Cancelled': 13, 'O_Create Offer': 14, 'O_Created': 15, 'O_Refused': 16, 'O_Returned': 17, 'O_Sent (mail and online)': 18, 'O_Sent (online only)': 19, 'W_Assess potential fraud': 20, 'W_Call after offers': 21, 'W_Call incomplete files': 22, 'W_Complete application': 23, 'W_Handle leads': 24, 'W_Personal Loan collection': 25, 'W_Shortened completion ': 26, 'W_Validate application': 27}), ('Action', 7, {'Created': 1, 'Deleted': 2, 'EOS': 3, 'Obtained': 4, 'Released': 5, 'statechange': 6}), ('org:resource', 150, {'EOS': 1, 'User_1': 2, 'User_10': 3, 'User_100': 4, 'User_101': 5, 'User_102': 6, 'User_103': 7, 'User_104': 8, 'User_105': 9, 'User_106': 10, 'User_107': 11, 'User_108': 12, 'User_109': 13, 'User_11': 14, 'User_110': 15, 'User_111': 16, 'User_112': 17, 'User_113': 18, 'User_114': 19, 'User_115': 20, 'User_116': 21, 'User_117': 22, 'User_118': 23, 'User_119': 24, 'User_12': 25, 'User_120': 26, 'User_121': 27, 'User_122': 28, 'User_123': 29, 'User_124': 30, 'User_125': 31, 'User_126': 32, 'User_127': 33, 'User_128': 34, 'User_129': 35, 'User_13': 36, 'User_130': 37, 'User_131': 38, 'User_132': 39, 'User_133': 40, 'User_134': 41, 'User_135': 42, 'User_136': 43, 'User_137': 44, 'User_138': 45, 'User_139': 46, 'User_14': 47, 'User_140': 48, 'User_141': 49, 'User_142': 50, 'User_143': 51, 'User_144': 52, 'User_145': 53, 'User_146': 54, 'User_148': 55, 'User_149': 56, 'User_15': 57, 'User_16': 58, 'User_17': 59, 'User_18': 60, 'User_19': 61, 'User_2': 62, 'User_20': 63, 'User_21': 64, 'User_22': 65, 'User_23': 66, 'User_24': 67, 'User_25': 68, 'User_26': 69, 'User_27': 70, 'User_28': 71, 'User_29': 72, 'User_3': 73, 'User_30': 74, 'User_31': 75, 'User_32': 76, 'User_33': 77, 'User_34': 78, 'User_35': 79, 'User_36': 80, 'User_37': 81, 'User_38': 82, 'User_39': 83, 'User_4': 84, 'User_40': 85, 'User_41': 86, 'User_42': 87, 'User_43': 88, 'User_44': 89, 'User_45': 90, 'User_46': 91, 'User_47': 92, 'User_48': 93, 'User_49': 94, 'User_5': 95, 'User_50': 96, 'User_51': 97, 'User_52': 98, 'User_53': 99, 'User_54': 100, 'User_55': 101, 'User_56': 102, 'User_57': 103, 'User_58': 104, 'User_59': 105, 'User_6': 106, 'User_60': 107, 'User_61': 108, 'User_62': 109, 'User_63': 110, 'User_64': 111, 'User_65': 112, 'User_66': 113, 'User_67': 114, 'User_68': 115, 'User_69': 116, 'User_7': 117, 'User_70': 118, 'User_71': 119, 'User_72': 120, 'User_73': 121, 'User_74': 122, 'User_75': 123, 'User_76': 124, 'User_77': 125, 'User_78': 126, 'User_79': 127, 'User_8': 128, 'User_80': 129, 'User_81': 130, 'User_82': 131, 'User_83': 132, 'User_84': 133, 'User_85': 134, 'User_86': 135, 'User_87': 136, 'User_88': 137, 'User_89': 138, 'User_9': 139, 'User_90': 140, 'User_91': 141, 'User_92': 142, 'User_93': 143, 'User_94': 144, 'User_95': 145, 'User_96': 146, 'User_97': 147, 'User_98': 148, 'User_99': 149}), ('EventOrigin', 5, {'Application': 1, 'EOS': 2, 'Offer': 3, 'Workflow': 4}), ('lifecycle:transition', 9, {'EOS': 1, 'ate_abort': 2, 'complete': 3, 'resume': 4, 'schedule': 5, 'start': 6, 'suspend': 7, 'withdraw': 8}), ('case:LoanGoal', 16, {'Boat': 1, 'Business goal': 2, 'Car': 3, 'Caravan / Camper': 4, 'Debt restructuring': 5, 'EOS': 6, 'Existing loan takeover': 7, 'Extra spending limit': 8, 'Home improvement': 9, 'Motorcycle': 10, 'Not speficied': 11, 'Other, see explanation': 12, 'Remaining debt home': 13, 'Tax payments': 14, 'Unknown': 15}), ('case:ApplicationType', 4, {'EOS': 1, 'Limit raise': 2, 'New credit': 3}), ('Accepted', 5, {'EOS': 1, 'False': 2, 'True': 3, nan: 4}), ('Selected', 5, {'EOS': 1, 'False': 2, 'True': 3, nan: 4})], [('case_elapsed_time', 1, {}), ('event_elapsed_time', 1, {}), ('day_in_week', 1, {}), ('seconds_in_day', 1, {}), ('case:RequestedAmount', 1, {}), ('FirstWithdrawalAmount', 1, {}), ('NumberOfTerms', 1, {}), ('MonthlyCost', 1, {}), ('CreditScore', 1, {})])\n",
            "Encoder input features:  [['concept:name', 'Action', 'org:resource', 'EventOrigin', 'lifecycle:transition', 'case:LoanGoal', 'case:ApplicationType', 'Accepted', 'Selected'], ['case_elapsed_time', 'event_elapsed_time', 'day_in_week', 'seconds_in_day', 'case:RequestedAmount', 'FirstWithdrawalAmount', 'NumberOfTerms', 'MonthlyCost', 'CreditScore']]\n",
            "Decoder input+output features:  [['concept:name', 'org:resource', 'lifecycle:transition'], ['case_elapsed_time', 'event_elapsed_time']]\n",
            "\n",
            "\n",
            "Sequence length of decoder output:  4\n",
            "\n",
            "\n",
            "Cells hidden size:  128\n",
            "Number of LSTM layer:  4\n",
            "Dropout rate:  0.1\n",
            "\n",
            "\n",
            "Encoder number of labels for each input feature (categorical, numerical):  [[28, 7, 150, 5, 9, 16, 4, 5, 5], [1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
            "Encoder indices of tensors in dataset used as input:  [[0, 1, 2, 3, 4, 5, 6, 7, 8], [0, 1, 2, 3, 4, 5, 6, 7, 8]]\n",
            "Embeddings encoder:  ModuleList(\n",
            "  (0): Embedding(28, 10)\n",
            "  (1): Embedding(7, 5)\n",
            "  (2): Embedding(150, 26)\n",
            "  (3): Embedding(5, 4)\n",
            "  (4): Embedding(9, 5)\n",
            "  (5): Embedding(16, 8)\n",
            "  (6): Embedding(4, 3)\n",
            "  (7-8): 2 x Embedding(5, 4)\n",
            ")\n",
            "Total embedding feature size encoder:  69\n",
            "Total numerical feature size encoder:  9\n",
            "Input feature size encoder:  78\n",
            "Encoder initialized! \n",
            "\n",
            "Decoder label values size for each categorical input feature:  [28, 150, 9]\n",
            "Decoder label values size for each numerical input feature:  [1, 1]\n",
            "Decoder indices of tensors in dataset used as input:  [[0, 2, 4], [0, 1]]\n",
            "Embeddings decoder:  ModuleList(\n",
            "  (0): Embedding(28, 10)\n",
            "  (1): Embedding(150, 26)\n",
            "  (2): Embedding(9, 5)\n",
            ")\n",
            "Total embedding feature size decoder:  41\n",
            "Total numerical feature size decoder:  2\n",
            "Input feature size decoder:  43\n",
            "Output feature list of dicts (featue name, feature output size) of decoder:  [{'concept:name': 28, 'org:resource': 150, 'lifecycle:transition': 9}, {'case_elapsed_time': 1, 'event_elapsed_time': 1}]\n",
            "Decoder initialized! \n",
            "\n",
            "Output feature list of dicts (featue name, tensor index in dataset) of decoder:  [{'concept:name': 0, 'org:resource': 2, 'lifecycle:transition': 4}, {'case_elapsed_time': 0, 'event_elapsed_time': 1}]\n",
            "Original dataset loaded: 185789 cases\n",
            "Perturbed dataset loaded: 185789 cases\n"
          ]
        }
      ],
      "source": [
        "# Load model\n",
        "#file_path_model = '../../../training_variational_dropout/sepsis/Sepsis_full_no_grad_norm_robustness.pkl'\n",
        "file_path_model = '../../../training_variational_dropout/BPIC17/BPIC_2017_setting_2.pkl'\n",
        "output_dir = '../../../../../evaluation_results/robustness/BPIC17/last_event_attack/'\n",
        "model = DropoutUncertaintyEncoderDecoderLSTM.load(file_path_model, dropout=0.1)\n",
        "\n",
        "file_path_original = '../../../../../encoded_data/BPIC17/BPIC_2017_all_5_val.pkl'\n",
        "file_path_perturbed = '../../../../../encoded_data/BPIC17/BPIC_2017_all_5_val.pkl'\n",
        "file_path_redo_activity = '../../../../../encoded_data/BPIC17/val.pkl'\n",
        "file_path_redo_activity_pert = '../../../../../encoded_data/BPIC17/last_event_attack.pkl'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "original_dataset = torch.load(file_path_original, weights_only=False)\n",
        "perturbed_dataset = torch.load(file_path_perturbed, weights_only=False)\n",
        "redo_activity_dataset = torch.load(file_path_redo_activity, weights_only=False)\n",
        "redo_activity_pert_dataset = torch.load(file_path_redo_activity_pert, weights_only=False)\n",
        "\n",
        "\n",
        "print(f\"Original dataset loaded: {len(original_dataset)} cases\")\n",
        "print(f\"Perturbed dataset loaded: {len(perturbed_dataset)} cases\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sampled 17161 observations (10.0%) from 171611 total observations\n",
            "Original dataset now has 17161 entries\n",
            "Perturbed dataset now has 17161 entries\n"
          ]
        }
      ],
      "source": [
        "# Sample 10% of observations for faster inference\n",
        "import random\n",
        "\n",
        "# Set random seed for reproducibility (optional)\n",
        "random.seed(42)\n",
        "\n",
        "# Get all keys from both datasets\n",
        "all_keys_orig = list(redo_activity_dataset.keys())\n",
        "all_keys_pert = list(redo_activity_pert_dataset.keys())\n",
        "\n",
        "# Ensure both datasets have the same keys\n",
        "assert set(all_keys_orig) == set(all_keys_pert), \"Datasets must have matching keys\"\n",
        "\n",
        "# Calculate 10% sample size\n",
        "sample_size = max(1, int(len(all_keys_orig) * 0.5))\n",
        "\n",
        "# Randomly sample 10% of the keys\n",
        "sampled_keys = random.sample(all_keys_orig, sample_size)\n",
        "\n",
        "# Create new dictionaries with only sampled keys\n",
        "redo_activity_dataset = {key: redo_activity_dataset[key] for key in sampled_keys}\n",
        "redo_activity_pert_dataset = {key: redo_activity_pert_dataset[key] for key in sampled_keys}\n",
        "\n",
        "print(f\"Sampled {len(sampled_keys)} observations ({len(sampled_keys)/len(all_keys_orig)*100:.1f}%) from {len(all_keys_orig)} total observations\")\n",
        "print(f\"Original dataset now has {len(redo_activity_dataset)} entries\")\n",
        "print(f\"Perturbed dataset now has {len(redo_activity_pert_dataset)} entries\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data set categories:  ([('concept:name', 28, {'A_Accepted': 1, 'A_Cancelled': 2, 'A_Complete': 3, 'A_Concept': 4, 'A_Create Application': 5, 'A_Denied': 6, 'A_Incomplete': 7, 'A_Pending': 8, 'A_Submitted': 9, 'A_Validating': 10, 'EOS': 11, 'O_Accepted': 12, 'O_Cancelled': 13, 'O_Create Offer': 14, 'O_Created': 15, 'O_Refused': 16, 'O_Returned': 17, 'O_Sent (mail and online)': 18, 'O_Sent (online only)': 19, 'W_Assess potential fraud': 20, 'W_Call after offers': 21, 'W_Call incomplete files': 22, 'W_Complete application': 23, 'W_Handle leads': 24, 'W_Personal Loan collection': 25, 'W_Shortened completion ': 26, 'W_Validate application': 27}), ('Action', 7, {'Created': 1, 'Deleted': 2, 'EOS': 3, 'Obtained': 4, 'Released': 5, 'statechange': 6}), ('org:resource', 150, {'EOS': 1, 'User_1': 2, 'User_10': 3, 'User_100': 4, 'User_101': 5, 'User_102': 6, 'User_103': 7, 'User_104': 8, 'User_105': 9, 'User_106': 10, 'User_107': 11, 'User_108': 12, 'User_109': 13, 'User_11': 14, 'User_110': 15, 'User_111': 16, 'User_112': 17, 'User_113': 18, 'User_114': 19, 'User_115': 20, 'User_116': 21, 'User_117': 22, 'User_118': 23, 'User_119': 24, 'User_12': 25, 'User_120': 26, 'User_121': 27, 'User_122': 28, 'User_123': 29, 'User_124': 30, 'User_125': 31, 'User_126': 32, 'User_127': 33, 'User_128': 34, 'User_129': 35, 'User_13': 36, 'User_130': 37, 'User_131': 38, 'User_132': 39, 'User_133': 40, 'User_134': 41, 'User_135': 42, 'User_136': 43, 'User_137': 44, 'User_138': 45, 'User_139': 46, 'User_14': 47, 'User_140': 48, 'User_141': 49, 'User_142': 50, 'User_143': 51, 'User_144': 52, 'User_145': 53, 'User_146': 54, 'User_148': 55, 'User_149': 56, 'User_15': 57, 'User_16': 58, 'User_17': 59, 'User_18': 60, 'User_19': 61, 'User_2': 62, 'User_20': 63, 'User_21': 64, 'User_22': 65, 'User_23': 66, 'User_24': 67, 'User_25': 68, 'User_26': 69, 'User_27': 70, 'User_28': 71, 'User_29': 72, 'User_3': 73, 'User_30': 74, 'User_31': 75, 'User_32': 76, 'User_33': 77, 'User_34': 78, 'User_35': 79, 'User_36': 80, 'User_37': 81, 'User_38': 82, 'User_39': 83, 'User_4': 84, 'User_40': 85, 'User_41': 86, 'User_42': 87, 'User_43': 88, 'User_44': 89, 'User_45': 90, 'User_46': 91, 'User_47': 92, 'User_48': 93, 'User_49': 94, 'User_5': 95, 'User_50': 96, 'User_51': 97, 'User_52': 98, 'User_53': 99, 'User_54': 100, 'User_55': 101, 'User_56': 102, 'User_57': 103, 'User_58': 104, 'User_59': 105, 'User_6': 106, 'User_60': 107, 'User_61': 108, 'User_62': 109, 'User_63': 110, 'User_64': 111, 'User_65': 112, 'User_66': 113, 'User_67': 114, 'User_68': 115, 'User_69': 116, 'User_7': 117, 'User_70': 118, 'User_71': 119, 'User_72': 120, 'User_73': 121, 'User_74': 122, 'User_75': 123, 'User_76': 124, 'User_77': 125, 'User_78': 126, 'User_79': 127, 'User_8': 128, 'User_80': 129, 'User_81': 130, 'User_82': 131, 'User_83': 132, 'User_84': 133, 'User_85': 134, 'User_86': 135, 'User_87': 136, 'User_88': 137, 'User_89': 138, 'User_9': 139, 'User_90': 140, 'User_91': 141, 'User_92': 142, 'User_93': 143, 'User_94': 144, 'User_95': 145, 'User_96': 146, 'User_97': 147, 'User_98': 148, 'User_99': 149}), ('EventOrigin', 5, {'Application': 1, 'EOS': 2, 'Offer': 3, 'Workflow': 4}), ('lifecycle:transition', 9, {'EOS': 1, 'ate_abort': 2, 'complete': 3, 'resume': 4, 'schedule': 5, 'start': 6, 'suspend': 7, 'withdraw': 8}), ('case:LoanGoal', 16, {'Boat': 1, 'Business goal': 2, 'Car': 3, 'Caravan / Camper': 4, 'Debt restructuring': 5, 'EOS': 6, 'Existing loan takeover': 7, 'Extra spending limit': 8, 'Home improvement': 9, 'Motorcycle': 10, 'Not speficied': 11, 'Other, see explanation': 12, 'Remaining debt home': 13, 'Tax payments': 14, 'Unknown': 15}), ('case:ApplicationType', 4, {'EOS': 1, 'Limit raise': 2, 'New credit': 3}), ('Accepted', 5, {'EOS': 1, 'False': 2, 'True': 3, nan: 4}), ('Selected', 5, {'EOS': 1, 'False': 2, 'True': 3, nan: 4})], [('case_elapsed_time', 1, {}), ('event_elapsed_time', 1, {}), ('day_in_week', 1, {}), ('seconds_in_day', 1, {}), ('case:RequestedAmount', 1, {}), ('FirstWithdrawalAmount', 1, {}), ('NumberOfTerms', 1, {}), ('MonthlyCost', 1, {}), ('CreditScore', 1, {})])\n",
            "Encoder input features:  [['concept:name', 'Action', 'org:resource', 'EventOrigin', 'lifecycle:transition', 'case:LoanGoal', 'case:ApplicationType', 'Accepted', 'Selected'], ['case_elapsed_time', 'event_elapsed_time', 'day_in_week', 'seconds_in_day', 'case:RequestedAmount', 'FirstWithdrawalAmount', 'NumberOfTerms', 'MonthlyCost', 'CreditScore']]\n",
            "Decoder input+output features:  [['concept:name', 'org:resource', 'lifecycle:transition'], ['case_elapsed_time', 'event_elapsed_time']]\n",
            "\n",
            "\n",
            "Sequence length of decoder output:  4\n",
            "\n",
            "\n",
            "Cells hidden size:  128\n",
            "Number of LSTM layer:  4\n",
            "Dropout rate:  0.1\n",
            "\n",
            "\n",
            "Encoder number of labels for each input feature (categorical, numerical):  [[28, 7, 150, 5, 9, 16, 4, 5, 5], [1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
            "Encoder indices of tensors in dataset used as input:  [[0, 1, 2, 3, 4, 5, 6, 7, 8], [0, 1, 2, 3, 4, 5, 6, 7, 8]]\n",
            "Embeddings encoder:  ModuleList(\n",
            "  (0): Embedding(28, 10)\n",
            "  (1): Embedding(7, 5)\n",
            "  (2): Embedding(150, 26)\n",
            "  (3): Embedding(5, 4)\n",
            "  (4): Embedding(9, 5)\n",
            "  (5): Embedding(16, 8)\n",
            "  (6): Embedding(4, 3)\n",
            "  (7-8): 2 x Embedding(5, 4)\n",
            ")\n",
            "Total embedding feature size encoder:  69\n",
            "Total numerical feature size encoder:  9\n",
            "Input feature size encoder:  78\n",
            "Encoder initialized! \n",
            "\n",
            "Decoder label values size for each categorical input feature:  [28, 150, 9]\n",
            "Decoder label values size for each numerical input feature:  [1, 1]\n",
            "Decoder indices of tensors in dataset used as input:  [[0, 2, 4], [0, 1]]\n",
            "Embeddings decoder:  ModuleList(\n",
            "  (0): Embedding(28, 10)\n",
            "  (1): Embedding(150, 26)\n",
            "  (2): Embedding(9, 5)\n",
            ")\n",
            "Total embedding feature size decoder:  41\n",
            "Total numerical feature size decoder:  2\n",
            "Input feature size decoder:  43\n",
            "Output feature list of dicts (featue name, feature output size) of decoder:  [{'concept:name': 28, 'org:resource': 150, 'lifecycle:transition': 9}, {'case_elapsed_time': 1, 'event_elapsed_time': 1}]\n",
            "Decoder initialized! \n",
            "\n",
            "Output feature list of dicts (featue name, tensor index in dataset) of decoder:  [{'concept:name': 0, 'org:resource': 2, 'lifecycle:transition': 4}, {'case_elapsed_time': 0, 'event_elapsed_time': 1}]\n"
          ]
        }
      ],
      "source": [
        "#tMp\n",
        "file_path_model = '../../../training_variational_dropout/BPIC17/BPIC_2017_setting_2.pkl'\n",
        "model = DropoutUncertaintyEncoderDecoderLSTM.load(file_path_model, dropout=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ProbabilisticEvaluation instances created\n"
          ]
        }
      ],
      "source": [
        "# Create evaluation instances (NON-RANDOM ORDER)\n",
        "eval_original = ProbabilisticEvaluation(\n",
        "    model, original_dataset,\n",
        "    concept_name='concept:name',\n",
        "    num_processes=1, \n",
        "    growing_num_values=['case_elapsed_time'],\n",
        "    samples_per_case=1000,\n",
        "    sample_argmax=False,\n",
        "    use_variance_cat=True,\n",
        "    use_variance_num=True,\n",
        "    all_cat=['concept:name', 'org:resource', 'lifecycle:transition'],\n",
        "    all_num=['case_elapsed_time', 'event_elapsed_time'],\n",
        "    dataset_predefined_prefixes=redo_activity_dataset\n",
        ")\n",
        "\n",
        "eval_perturbed = ProbabilisticEvaluation(\n",
        "    model, perturbed_dataset,\n",
        "    concept_name='concept:name', #'Activity'\n",
        "    num_processes=1,\n",
        "    growing_num_values=['case_elapsed_time'],\n",
        "    samples_per_case=1000,\n",
        "    sample_argmax=False,\n",
        "    use_variance_cat=True,\n",
        "    use_variance_num=True,\n",
        "    all_cat=['concept:name', 'org:resource', 'lifecycle:transition'],\n",
        "    all_num=['case_elapsed_time', 'event_elapsed_time'],\n",
        "    dataset_predefined_prefixes=redo_activity_pert_dataset\n",
        ")\n",
        "\n",
        "print(\"ProbabilisticEvaluation instances created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Robustness metrics module imported\n"
          ]
        }
      ],
      "source": [
        "# Import robustness metrics module\n",
        "import robustness.evaluator.robustness_metrics\n",
        "#importlib.reload(robustness.robustness_metrics)\n",
        "from robustness.evaluator.robustness_metrics import save_chunk\n",
        "\n",
        "print(\"Robustness metrics module imported\")\n",
        "\n",
        "# Helper functions for filtering predictions and calculating remaining time\n",
        "def filter_prediction_events(prediction_list, concept_name='concept:name'):\n",
        "    \"\"\"Filter prediction events to only include concept:name and case_elapsed_time\"\"\"\n",
        "    if prediction_list is None:\n",
        "        return None\n",
        "    filtered = []\n",
        "    for event in prediction_list:\n",
        "        if not isinstance(event, dict):\n",
        "            continue\n",
        "        filtered_event = {}\n",
        "        if concept_name in event:\n",
        "            filtered_event[concept_name] = event[concept_name]\n",
        "        if 'case_elapsed_time' in event:\n",
        "            filtered_event['case_elapsed_time'] = event['case_elapsed_time']\n",
        "        filtered.append(filtered_event)\n",
        "    return filtered\n",
        "\n",
        "def calculate_remaining_time(prefix, prediction, concept_name='concept:name'):\n",
        "    \"\"\"Calculate remaining time from prefix and prediction\"\"\"\n",
        "    if not prefix or not prediction:\n",
        "        return None\n",
        "    if not isinstance(prefix[-1], dict) or 'case_elapsed_time' not in prefix[-1]:\n",
        "        return None\n",
        "    if not isinstance(prediction[-1], dict) or 'case_elapsed_time' not in prediction[-1]:\n",
        "        return None\n",
        "    current_time = prefix[-1]['case_elapsed_time']\n",
        "    final_time = prediction[-1]['case_elapsed_time']\n",
        "    return final_time - current_time\n",
        "\n",
        "def calculate_sampled_remaining_times(prefix, predicted_suffixes, concept_name='concept:name'):\n",
        "    \"\"\"Calculate remaining time for each sample in predicted_suffixes\"\"\"\n",
        "    if not prefix or not predicted_suffixes:\n",
        "        return None\n",
        "    if not isinstance(prefix[-1], dict) or 'case_elapsed_time' not in prefix[-1]:\n",
        "        return None\n",
        "    current_time = prefix[-1]['case_elapsed_time']\n",
        "    \n",
        "    remaining_times = []\n",
        "    for sample in predicted_suffixes:\n",
        "        if not sample or not isinstance(sample[-1], dict) or 'case_elapsed_time' not in sample[-1]:\n",
        "            remaining_times.append(None)\n",
        "        else:\n",
        "            final_time = sample[-1]['case_elapsed_time']\n",
        "            remaining_times.append(final_time - current_time)\n",
        "    return remaining_times\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c6a165cbd35e48c1b20000e13dbe80ba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating robustness: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "922021937bad440aad27f05106d5528e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/17161 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "18dfdef22f3746cdb17d6547919829d6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/17161 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved 50 results to ../../../../../evaluation_results/robustness/BPIC17/last_event_attack/robustness_results_part_050.pkl\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m results \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      6\u001b[0m concept_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconcept:name\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Match the concept_name used in ProbabilisticEvaluation\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcase_name_orig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix_len_orig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix_orig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicted_suffixes_orig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix_orig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean_pred_orig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m----> 9\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcase_name_pert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix_len_pert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix_pert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicted_suffixes_pert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix_pert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean_pred_pert\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43meval_original\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_with_predifined_prefix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_order\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m             \u001b[49m\u001b[43meval_perturbed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_with_predifined_prefix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_order\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m         \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluating robustness\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Ensure we're comparing the same case and prefix length\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01massert\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcase_name_orig\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcase_name_pert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCase mismatch: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcase_name_orig\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m != \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcase_name_pert\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01massert\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprefix_len_orig\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprefix_len_pert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPrefix length mismatch: \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix_len_orig\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m != \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix_len_pert\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n",
            "File \u001b[0;32m~/henryks_students/leon_urny/Robustness-in-suffix-prediction/.venv/lib64/python3.13/site-packages/tqdm/notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 250\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# return super(tqdm...) will not catch exception\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
            "File \u001b[0;32m~/henryks_students/leon_urny/Robustness-in-suffix-prediction/.venv/lib64/python3.13/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
            "File \u001b[0;32m~/henryks_students/leon_urny/Robustness-in-suffix-prediction/ml_models/notebooks/evaluation_run_notebooks/normal_4layer/BPIC17/../../../../evaluation/probabilistic_evaluation.py:205\u001b[0m, in \u001b[0;36mProbabilisticEvaluation.evaluate_with_predifined_prefix\u001b[0;34m(self, random_order, include_model_states)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# Create list of indices for potential randomization\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m# if random_order:\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m#     items = random.sample(items, len(items))\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# Iterate over predefined prefix-suffix pairs\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (case_name, prefix_len), (prefix, suffix) \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_predefined_prefixes\u001b[38;5;241m.\u001b[39mitems()):\n\u001b[0;32m--> 205\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcase_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_model_states\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/henryks_students/leon_urny/Robustness-in-suffix-prediction/ml_models/notebooks/evaluation_run_notebooks/normal_4layer/BPIC17/../../../../evaluation/probabilistic_evaluation.py:168\u001b[0m, in \u001b[0;36mProbabilisticEvaluation._evaluate_single\u001b[0;34m(self, case_name, prefix_len, prefix, suffix, include_model_states)\u001b[0m\n\u001b[1;32m    165\u001b[0m readable_suffix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcase_to_readable(suffix, prune_eos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# print(\"Suffix: \", readable_suffix)\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m mean_prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_suffix_with_means\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# print(\"Mean Pred: \", mean_prediction)\u001b[39;00m\n\u001b[1;32m    171\u001b[0m predicted_suffixes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_probabilistic_suffix(prefix, prefix_len, include_model_states)\n",
            "File \u001b[0;32m~/henryks_students/leon_urny/Robustness-in-suffix-prediction/ml_models/notebooks/evaluation_run_notebooks/normal_4layer/BPIC17/../../../../evaluation/evaluation.py:127\u001b[0m, in \u001b[0;36mEvaluation._predict_suffix_with_means\u001b[0;34m(self, prefix, prefix_len)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdecoder \n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# Prediction by model\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m prediction, (h, c), z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m suffix \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    130\u001b[0m max_iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mencoder_decoder\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mencoder_decoder\u001b[38;5;241m.\u001b[39mmin_suffix_size \u001b[38;5;241m-\u001b[39m prefix_len\n",
            "File \u001b[0;32m~/henryks_students/leon_urny/Robustness-in-suffix-prediction/ml_models/notebooks/evaluation_run_notebooks/normal_4layer/BPIC17/../../../../model/dropout_uncertainty_enc_dec_LSTM/dropout_uncertainty_model.py:412\u001b[0m, in \u001b[0;36mDropoutUncertaintyEncoderDecoderLSTM.inference\u001b[0;34m(self, prefix, last_event, hx, z)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;66;03m# First Prediciton\u001b[39;00m\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    411\u001b[0m         \u001b[38;5;66;03m# Call encoder\u001b[39;00m\n\u001b[0;32m--> 412\u001b[0m         (h_enc, c_enc) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    414\u001b[0m         \u001b[38;5;66;03m# Get SOS event: Last prefx event:\u001b[39;00m\n\u001b[1;32m    415\u001b[0m         cat_prefixes, num_prefixes \u001b[38;5;241m=\u001b[39m prefix\n",
            "File \u001b[0;32m~/henryks_students/leon_urny/Robustness-in-suffix-prediction/.venv/lib64/python3.13/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/henryks_students/leon_urny/Robustness-in-suffix-prediction/.venv/lib64/python3.13/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/henryks_students/leon_urny/Robustness-in-suffix-prediction/ml_models/notebooks/evaluation_run_notebooks/normal_4layer/BPIC17/../../../../model/dropout_uncertainty_enc_dec_LSTM/dropout_uncertainty_encoder.py:80\u001b[0m, in \u001b[0;36mDropoutUncertaintyLSTMEncoder.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Pass through the remaining LSTM cell: Layer gets for: input: h_n Tensor, hx: (h, c)\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_layers):\n\u001b[0;32m---> 80\u001b[0m     outputs, (h, c), _ \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (h, c)\n",
            "File \u001b[0;32m~/henryks_students/leon_urny/Robustness-in-suffix-prediction/.venv/lib64/python3.13/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/henryks_students/leon_urny/Robustness-in-suffix-prediction/.venv/lib64/python3.13/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/henryks_students/leon_urny/Robustness-in-suffix-prediction/ml_models/notebooks/evaluation_run_notebooks/normal_4layer/BPIC17/../../../../model/dropout_uncertainty_enc_dec_LSTM/dropout_uncertainty_LSTM_cell.py:192\u001b[0m, in \u001b[0;36mDropoutUncertaintyLSTMCell.forward\u001b[0;34m(self, input, hx, z)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# Forget gate: Which information from previous step is kept and which thrown away\u001b[39;00m\n\u001b[1;32m    191\u001b[0m f \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWf(x_f) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mUf(h_f))\n\u001b[0;32m--> 192\u001b[0m c_tilde \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_c\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mUc(h_c))\n\u001b[1;32m    193\u001b[0m o \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWo(x_o) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mUo(h_o))\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# Updated cell state\u001b[39;00m\n",
            "File \u001b[0;32m~/henryks_students/leon_urny/Robustness-in-suffix-prediction/.venv/lib64/python3.13/site-packages/torch/nn/modules/module.py:1732\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1729\u001b[0m             tracing_state\u001b[38;5;241m.\u001b[39mpop_scope()\n\u001b[1;32m   1730\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m-> 1732\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapped_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1734\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Main evaluation loop\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "save_every = 50\n",
        "results = {}\n",
        "concept_name = 'concept:name'  # Match the concept_name used in ProbabilisticEvaluation\n",
        "\n",
        "for i, ((case_name_orig, prefix_len_orig, prefix_orig, predicted_suffixes_orig, suffix_orig, mean_pred_orig),\n",
        "        (case_name_pert, prefix_len_pert, prefix_pert, predicted_suffixes_pert, suffix_pert, mean_pred_pert)) in enumerate(\n",
        "    tqdm(zip(eval_original.evaluate_with_predifined_prefix(random_order=False), \n",
        "             eval_perturbed.evaluate_with_predifined_prefix(random_order=False)), \n",
        "         desc=\"Evaluating robustness\")):\n",
        "    \n",
        "    # Ensure we're comparing the same case and prefix length\n",
        "    assert case_name_orig == case_name_pert, f\"Case mismatch: {case_name_orig} != {case_name_pert}\"\n",
        "\n",
        "    assert prefix_len_orig == prefix_len_pert, f\"Prefix length mismatch: {prefix_len_orig} != {prefix_len_pert}\"\n",
        "\n",
        "    # Filter predictions to only include concept:name and case_elapsed_time\n",
        "    mean_pred_orig_filtered = filter_prediction_events(mean_pred_orig, concept_name=concept_name)\n",
        "    predicted_suffixes_orig_filtered = [filter_prediction_events(sample, concept_name=concept_name) \n",
        "                                        for sample in predicted_suffixes_orig] if predicted_suffixes_orig else None\n",
        "    \n",
        "    mean_pred_pert_filtered = filter_prediction_events(mean_pred_pert, concept_name=concept_name)\n",
        "    predicted_suffixes_pert_filtered = [filter_prediction_events(sample, concept_name=concept_name) \n",
        "                                        for sample in predicted_suffixes_pert] if predicted_suffixes_pert else None\n",
        "    \n",
        "    # Calculate remaining times immediately\n",
        "    mean_pred_remaining_time_orig = calculate_remaining_time(prefix_orig, mean_pred_orig, concept_name=concept_name)\n",
        "    sampled_remaining_time_orig = calculate_sampled_remaining_times(prefix_orig, predicted_suffixes_orig, concept_name=concept_name)\n",
        "    \n",
        "    mean_pred_remaining_time_pert = calculate_remaining_time(prefix_pert, mean_pred_pert, concept_name=concept_name)\n",
        "    sampled_remaining_time_pert = calculate_sampled_remaining_times(prefix_pert, predicted_suffixes_pert, concept_name=concept_name)\n",
        "\n",
        "    # Store results with new structure\n",
        "    key = (case_name_orig, prefix_len_orig)\n",
        "    results[key] = {\n",
        "        'original': (\n",
        "            prefix_orig,  # Keep all fields\n",
        "            suffix_orig,  # Keep all fields\n",
        "            mean_pred_orig_filtered,  # Filtered: only concept:name and case_elapsed_time\n",
        "            predicted_suffixes_orig_filtered,  # Filtered: only concept:name and case_elapsed_time\n",
        "            mean_pred_remaining_time_orig,  # NEW: single float\n",
        "            sampled_remaining_time_orig  # NEW: list of floats\n",
        "        ),\n",
        "        'perturbed': (\n",
        "            prefix_pert,  # Keep all fields\n",
        "            suffix_pert,  # Keep all fields\n",
        "            mean_pred_pert_filtered,  # Filtered: only concept:name and case_elapsed_time\n",
        "            predicted_suffixes_pert_filtered,  # Filtered: only concept:name and case_elapsed_time\n",
        "            mean_pred_remaining_time_pert,  # NEW: single float\n",
        "            sampled_remaining_time_pert  # NEW: list of floats\n",
        "        ),\n",
        "    }\n",
        "\n",
        "    \n",
        "    if (i + 1) % save_every == 0:\n",
        "        save_chunk(results, i, output_dir)\n",
        "        results = {}\n",
        "\n",
        "if len(results):\n",
        "    save_chunk(results, i, output_dir)\n",
        "\n",
        "print(\"Robustness evaluation completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 12 chunk files\n",
            "Loading robustness_results_part_050.pkl...\n",
            "  Added 50 results from robustness_results_part_050.pkl\n",
            "Loading robustness_results_part_100.pkl...\n",
            "  Added 50 results from robustness_results_part_100.pkl\n",
            "Loading robustness_results_part_150.pkl...\n",
            "  Added 50 results from robustness_results_part_150.pkl\n",
            "Loading robustness_results_part_200.pkl...\n",
            "  Added 50 results from robustness_results_part_200.pkl\n",
            "Loading robustness_results_part_250.pkl...\n",
            "  Added 50 results from robustness_results_part_250.pkl\n",
            "Loading robustness_results_part_300.pkl...\n",
            "  Added 50 results from robustness_results_part_300.pkl\n",
            "Loading robustness_results_part_350.pkl...\n",
            "  Added 50 results from robustness_results_part_350.pkl\n",
            "Loading robustness_results_part_400.pkl...\n",
            "  Added 50 results from robustness_results_part_400.pkl\n",
            "Loading robustness_results_part_450.pkl...\n",
            "  Added 50 results from robustness_results_part_450.pkl\n",
            "Loading robustness_results_part_500.pkl...\n",
            "  Added 50 results from robustness_results_part_500.pkl\n",
            "Loading robustness_results_part_550.pkl...\n",
            "  Added 50 results from robustness_results_part_550.pkl\n",
            "Loading robustness_results_part_600.pkl...\n",
            "  Added 50 results from robustness_results_part_600.pkl\n",
            "Adding final 1 results...\n",
            "\n",
            "Total results loaded: 601\n",
            "Combined results saved to ../../../../../evaluation_results/robustness/BPIC17/last_event_attack/robustness_results.pkl\n"
          ]
        }
      ],
      "source": [
        "# Load all saved chunks and combine them\n",
        "all_results = {}\n",
        "# Get all chunk files and sort them\n",
        "chunk_files = [f for f in os.listdir(output_dir) if f.startswith('robustness_results_part_')]\n",
        "chunk_files.sort()  # Ensure correct order\n",
        "\n",
        "print(f\"Found {len(chunk_files)} chunk files\")\n",
        "\n",
        "for chunk_file in chunk_files:\n",
        "    chunk_path = os.path.join(output_dir, chunk_file)\n",
        "    print(f\"Loading {chunk_file}...\")\n",
        "    with open(chunk_path, 'rb') as f:\n",
        "        chunk_results = pickle.load(f)\n",
        "        all_results.update(chunk_results)\n",
        "        print(f\"  Added {len(chunk_results)} results from {chunk_file}\")\n",
        "\n",
        "# Also add the final results if any (e.g. from a still-running evaluation loop)\n",
        "if 'results' in locals() and len(results) > 0:\n",
        "    print(f\"Adding final {len(results)} results...\")\n",
        "    all_results.update(results)\n",
        "\n",
        "print(f\"\\nTotal results loaded: {len(all_results)}\")\n",
        "\n",
        "# Save combined results into a single pickle file\n",
        "combined_results_path = os.path.join(output_dir, 'robustness_results.pkl')\n",
        "with open(combined_results_path, 'wb') as f:\n",
        "    pickle.dump(all_results, f)\n",
        "\n",
        "print(f\"Combined results saved to {combined_results_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
