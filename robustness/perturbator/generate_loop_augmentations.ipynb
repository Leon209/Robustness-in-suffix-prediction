{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "533a26b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "sys.path.insert(1, '../..')\n",
    "sys.path.insert(0, \"../src\")  # src package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8c82f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from activity_pertubator import get_train_val_test\n",
    "# get_train_val_test(\n",
    "#     csv_path= \"../data/helpdesk.csv\",\n",
    "#     train_size= 0.7,\n",
    "#     val_size = 0.15,\n",
    "#     test_size= 0.15,\n",
    "#     output_dir=\"../data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0af28d",
   "metadata": {},
   "source": [
    "# Learn and Match Realistic Loops\n",
    "\n",
    "This section demonstrates the new approach:\n",
    "1. Learn realistic loops from a dataset (e.g., training data)\n",
    "2. Greedily match and insert learned loops into a new dataset (e.g., test data)\n",
    "3. Automatically split into prefixes/suffixes where loops are entirely in prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ef2765",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leonurny/Desktop/Robustness-in-suffix-prediction/robustness/../src/event_log_loader/new_event_log_loader.py:111: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.df = self.df.groupby(self.case_name).apply(min_timestamp_before).reset_index(drop=True)\n",
      "/Users/leonurny/Desktop/Robustness-in-suffix-prediction/robustness/../src/event_log_loader/new_event_log_loader.py:111: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  self.df = self.df.groupby(self.case_name).apply(min_timestamp_before).reset_index(drop=True)\n",
      "/Users/leonurny/Desktop/Robustness-in-suffix-prediction/robustness/../src/event_log_loader/new_event_log_loader.py:76: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  self.df = self.df.groupby(self.case_name, group_keys=False).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning loops from training data with 30919 rows...\n",
      "Found 933 loops to use for augmentation\n",
      "Sample loops (first 3):\n",
      "  Loop 1: Starting activity 'Take in charge ticket', sequence: ['Take in charge ticket', 'Wait', 'Take in charge ticket']\n",
      "  Loop 2: Starting activity 'Take in charge ticket', sequence: ['Take in charge ticket', 'Wait', 'Take in charge ticket']\n",
      "  Loop 3: Starting activity 'Take in charge ticket', sequence: ['Take in charge ticket', 'Wait', 'Take in charge ticket']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leonurny/Desktop/Robustness-in-suffix-prediction/robustness/../src/event_log_loader/new_event_log_loader.py:111: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  self.df = self.df.groupby(self.case_name).apply(min_timestamp_before).reset_index(drop=True)\n",
      "/Users/leonurny/Desktop/Robustness-in-suffix-prediction/robustness/../src/event_log_loader/new_event_log_loader.py:76: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  self.df = self.df.groupby(self.case_name, group_keys=False).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying learned loops to test data with 11323 rows...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mApplying learned loops to test data with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_test)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Match loops greedily and split into prefixes/suffixes\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m data = \u001b[43mmatch_loops_greedy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_new\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearned_loops\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearned_loops\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproperties\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprops\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_suffix_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mactivity_column\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprops\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconcept_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43meos_value\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEOS\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGenerated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m prefix/suffix pairs with loops in prefixes\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Robustness-in-suffix-prediction/robustness/activity_pertubator.py:520\u001b[39m, in \u001b[36mmatch_loops_greedy\u001b[39m\u001b[34m(data_new, learned_loops, properties, min_suffix_size, activity_column, eos_value)\u001b[39m\n\u001b[32m    504\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmatch_loops_greedy\u001b[39m(\n\u001b[32m    505\u001b[39m     data_new: pd.DataFrame,\n\u001b[32m    506\u001b[39m     learned_loops: List[Tuple[\u001b[38;5;28mstr\u001b[39m, pd.DataFrame]],\n\u001b[32m   (...)\u001b[39m\u001b[32m    510\u001b[39m     eos_value: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mEOS\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    511\u001b[39m ) -> Dict[Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m], Tuple[pd.DataFrame, pd.DataFrame]]:\n\u001b[32m    512\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    513\u001b[39m \u001b[33;03m    Greedily match and insert learned loops into a new dataset, then split into prefixes/suffixes.\u001b[39;00m\n\u001b[32m    514\u001b[39m \u001b[33;03m    \u001b[39;00m\n\u001b[32m    515\u001b[39m \u001b[33;03m    Loops are inserted greedily: take the first loop, find first matching starting activity,\u001b[39;00m\n\u001b[32m    516\u001b[39m \u001b[33;03m    insert it, then move to next loop. After insertion, split into prefixes/suffixes where:\u001b[39;00m\n\u001b[32m    517\u001b[39m \u001b[33;03m    - Loop must be entirely in prefix\u001b[39;00m\n\u001b[32m    518\u001b[39m \u001b[33;03m    - At least one more activity after loop completes\u001b[39;00m\n\u001b[32m    519\u001b[39m \u001b[33;03m    - Suffix has at least min_suffix_size non-EOS activities\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m520\u001b[39m \u001b[33;03m    \u001b[39;00m\n\u001b[32m    521\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    522\u001b[39m \u001b[33;03m        data_new: New dataset DataFrame (with all columns)\u001b[39;00m\n\u001b[32m    523\u001b[39m \u001b[33;03m        learned_loops: List of (starting_activity, loop_dataframe) tuples from learn_realistic_loops\u001b[39;00m\n\u001b[32m    524\u001b[39m \u001b[33;03m        properties: Event log properties dict\u001b[39;00m\n\u001b[32m    525\u001b[39m \u001b[33;03m        min_suffix_size: Minimum number of non-EOS activities required in suffix\u001b[39;00m\n\u001b[32m    526\u001b[39m \u001b[33;03m        activity_column: Name of activity column (defaults to properties[\"concept_name\"])\u001b[39;00m\n\u001b[32m    527\u001b[39m \u001b[33;03m        eos_value: Value identifying EOS rows\u001b[39;00m\n\u001b[32m    528\u001b[39m \u001b[33;03m        \u001b[39;00m\n\u001b[32m    529\u001b[39m \u001b[33;03m    Returns:\u001b[39;00m\n\u001b[32m    530\u001b[39m \u001b[33;03m        Dictionary keyed by (case_id, prefix_len) -> (prefix_df, suffix_df).\u001b[39;00m\n\u001b[32m    531\u001b[39m \u001b[33;03m        prefix_len counts only non-EOS rows.\u001b[39;00m\n\u001b[32m    532\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    533\u001b[39m     case_col = properties[\u001b[33m\"\u001b[39m\u001b[33mcase_name\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    534\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m activity_column \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Robustness-in-suffix-prediction/robustness/activity_pertubator.py:464\u001b[39m, in \u001b[36m_insert_loop_into_case\u001b[39m\u001b[34m(case_df, loop_data, loop_time_deltas, matching_activity, properties, activity_column, eos_value)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Robustness-in-suffix-prediction/robustness/activity_pertubator.py:676\u001b[39m, in \u001b[36m_update_day_and_seconds_features\u001b[39m\u001b[34m(df, timestamp_col, day_col, seconds_col)\u001b[39m\n\u001b[32m    669\u001b[39m     _update_day_and_seconds_features(updated_suffix, timestamp_col, day_col, seconds_col)\n\u001b[32m    671\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m updated_prefix, updated_suffix\n\u001b[32m    674\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_update_day_and_seconds_features\u001b[39m(\n\u001b[32m    675\u001b[39m     df: pd.DataFrame,\n\u001b[32m--> \u001b[39m\u001b[32m676\u001b[39m     timestamp_col: Optional[\u001b[38;5;28mstr\u001b[39m],\n\u001b[32m    677\u001b[39m     day_col: Optional[\u001b[38;5;28mstr\u001b[39m],\n\u001b[32m    678\u001b[39m     seconds_col: Optional[\u001b[38;5;28mstr\u001b[39m],\n\u001b[32m    679\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    680\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Update day-of-week and seconds-in-day columns based on timestamps.\"\"\"\u001b[39;00m\n\u001b[32m    681\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timestamp_col \u001b[38;5;129;01mand\u001b[39;00m timestamp_col \u001b[38;5;129;01min\u001b[39;00m df.columns:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from activity_pertubator import (\n",
    "    build_readable_event_log,\n",
    "    learn_realistic_loops,\n",
    "    match_loops_greedy,\n",
    ")\n",
    "\n",
    "# Load properties\n",
    "df_train, props = build_readable_event_log(\n",
    "    csv_path=\"../../data/helpdesk_train.csv\",  # Use training data to learn loops\n",
    "    properties_path=\"../../encoded_data/data_encoder/helpdesk_event_log_properties.pkl\",\n",
    ")\n",
    "\n",
    "print(f\"Learning loops from training data with {len(df_train)} rows...\")\n",
    "\n",
    "# Learn realistic loops from training data\n",
    "learned_loops = learn_realistic_loops(\n",
    "    df_train,\n",
    "    properties=props,\n",
    "    activity_column=props[\"concept_name\"],\n",
    "    eos_value=\"EOS\",\n",
    ")\n",
    "\n",
    "print(f\"Found {len(learned_loops)} loops to use for augmentation\")\n",
    "print(f\"Sample loops (first 3):\")\n",
    "for i, (starting_activity, loop_df) in enumerate(learned_loops[:3]):\n",
    "    activities = loop_df[props[\"concept_name\"]].tolist()\n",
    "    print(f\"  Loop {i+1}: Starting activity '{starting_activity}', sequence: {activities}\")\n",
    "\n",
    "# Load test data for augmentation\n",
    "df_test, _ = build_readable_event_log(\n",
    "    csv_path=\"../data/helpdesk_test.csv\",  # Use test data to insert loops\n",
    "    properties_path=\"../encoded_data/data_encoder/helpdesk_event_log_properties.pkl\",\n",
    ")\n",
    "\n",
    "print(f\"\\nApplying learned loops to test data with {len(df_test)} rows...\")\n",
    "\n",
    "# Match loops greedily and split into prefixes/suffixes\n",
    "data = match_loops_greedy(\n",
    "    data_new=df_test,\n",
    "    learned_loops=learned_loops,\n",
    "    properties=props,\n",
    "    min_suffix_size=props.get(2),\n",
    "    activity_column=props[\"concept_name\"],\n",
    "    eos_value=\"EOS\",\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(data)} prefix/suffix pairs with loops in prefixes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03bab33",
   "metadata": {},
   "source": [
    "# Redo Last Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "169f5ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leonurny/Desktop/Robustness-in-suffix-prediction/robustness/perturbator/../../ml_models/event_log_loader/new_event_log_loader.py:111: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  self.df = self.df.groupby(self.case_name).apply(min_timestamp_before).reset_index(drop=True)\n",
      "/Users/leonurny/Desktop/Robustness-in-suffix-prediction/robustness/perturbator/../../ml_models/event_log_loader/new_event_log_loader.py:76: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  self.df = self.df.groupby(self.case_name, group_keys=False).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11323\n",
      "Applied redo_last_activity_of_prefix to 3091 pairs\n"
     ]
    }
   ],
   "source": [
    "from activity_pertubator import (\n",
    "    build_readable_event_log,\n",
    "    split_prefix_suffix_readable,\n",
    "    redo_last_activity_of_prefix)\n",
    "\n",
    "\n",
    "df, props = build_readable_event_log(\n",
    "    csv_path=\"../../data/helpdesk_test.csv\",\n",
    "    properties_path=\"../../encoded_data/data_encoder/helpdesk_event_log_properties.pkl\",\n",
    ")\n",
    "\n",
    "print(len(df))\n",
    "\n",
    "data = split_prefix_suffix_readable(\n",
    "    df,\n",
    "    case_column=props[\"case_name\"],\n",
    "    activity_column=props[\"concept_name\"],\n",
    "    min_suffix_size=2,\n",
    ")\n",
    "\n",
    "# # Apply \"redo last activity\" augmentation to each prefix/suffix pair\n",
    "# augmented_data = {}\n",
    "# for key, (prefix_df, suffix_df) in data.items():\n",
    "#     new_prefix, new_suffix = redo_last_activity_of_prefix(\n",
    "#         prefix_df,\n",
    "#         suffix_df,\n",
    "#         properties=props,\n",
    "#         time_increment_seconds=60.0,\n",
    "#     )\n",
    "#     augmented_data[key] = (new_prefix, new_suffix)\n",
    "# data = augmented_data\n",
    "\n",
    "print(f\"Applied redo_last_activity_of_prefix to {len(data)} pairs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7ab2c597",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix, suffix =data[(\"Case 3788\", 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3556af9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case ID</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Resource</th>\n",
       "      <th>Complete Timestamp</th>\n",
       "      <th>Variant</th>\n",
       "      <th>Variant index</th>\n",
       "      <th>Variant.1</th>\n",
       "      <th>seriousness</th>\n",
       "      <th>customer</th>\n",
       "      <th>product</th>\n",
       "      <th>responsible_section</th>\n",
       "      <th>seriousness_2</th>\n",
       "      <th>service_level</th>\n",
       "      <th>service_type</th>\n",
       "      <th>support_section</th>\n",
       "      <th>workgroup</th>\n",
       "      <th>case_elapsed_time</th>\n",
       "      <th>event_elapsed_time</th>\n",
       "      <th>day_in_week</th>\n",
       "      <th>seconds_in_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Case 3788</td>\n",
       "      <td>Take in charge ticket</td>\n",
       "      <td>Value 2</td>\n",
       "      <td>2010-10-07 08:45:20</td>\n",
       "      <td>Variant 4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Variant 4</td>\n",
       "      <td>Value 1</td>\n",
       "      <td>Value 227</td>\n",
       "      <td>Value 3</td>\n",
       "      <td>Value 1</td>\n",
       "      <td>Value 1</td>\n",
       "      <td>Value 2</td>\n",
       "      <td>Value 1</td>\n",
       "      <td>Value 1</td>\n",
       "      <td>Value 1</td>\n",
       "      <td>4129.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>31520.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Case 3788</td>\n",
       "      <td>Resolve ticket</td>\n",
       "      <td>Value 7</td>\n",
       "      <td>2010-10-07 10:15:03</td>\n",
       "      <td>Variant 4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Variant 4</td>\n",
       "      <td>Value 1</td>\n",
       "      <td>Value 227</td>\n",
       "      <td>Value 3</td>\n",
       "      <td>Value 1</td>\n",
       "      <td>Value 1</td>\n",
       "      <td>Value 2</td>\n",
       "      <td>Value 1</td>\n",
       "      <td>Value 1</td>\n",
       "      <td>Value 1</td>\n",
       "      <td>9512.0</td>\n",
       "      <td>5383.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>36903.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Case 3788</td>\n",
       "      <td>Closed</td>\n",
       "      <td>Value 5</td>\n",
       "      <td>2010-11-24 08:40:24</td>\n",
       "      <td>Variant 4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Variant 4</td>\n",
       "      <td>Value 1</td>\n",
       "      <td>Value 227</td>\n",
       "      <td>Value 3</td>\n",
       "      <td>Value 1</td>\n",
       "      <td>Value 1</td>\n",
       "      <td>Value 2</td>\n",
       "      <td>Value 1</td>\n",
       "      <td>Value 1</td>\n",
       "      <td>Value 1</td>\n",
       "      <td>4151033.0</td>\n",
       "      <td>4141521.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>31224.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Case 3788</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>NaT</td>\n",
       "      <td>EOS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Case 3788</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>NaT</td>\n",
       "      <td>EOS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Case 3788</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>NaT</td>\n",
       "      <td>EOS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Case 3788</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>NaT</td>\n",
       "      <td>EOS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Case 3788</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>NaT</td>\n",
       "      <td>EOS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>EOS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Case ID               Activity Resource  Complete Timestamp    Variant  \\\n",
       "2  Case 3788  Take in charge ticket  Value 2 2010-10-07 08:45:20  Variant 4   \n",
       "3  Case 3788         Resolve ticket  Value 7 2010-10-07 10:15:03  Variant 4   \n",
       "4  Case 3788                 Closed  Value 5 2010-11-24 08:40:24  Variant 4   \n",
       "5  Case 3788                    EOS      EOS                 NaT        EOS   \n",
       "6  Case 3788                    EOS      EOS                 NaT        EOS   \n",
       "7  Case 3788                    EOS      EOS                 NaT        EOS   \n",
       "8  Case 3788                    EOS      EOS                 NaT        EOS   \n",
       "9  Case 3788                    EOS      EOS                 NaT        EOS   \n",
       "\n",
       "  Variant index  Variant.1 seriousness   customer  product  \\\n",
       "2           4.0  Variant 4     Value 1  Value 227  Value 3   \n",
       "3           4.0  Variant 4     Value 1  Value 227  Value 3   \n",
       "4           4.0  Variant 4     Value 1  Value 227  Value 3   \n",
       "5           NaN        EOS         EOS        EOS      EOS   \n",
       "6           NaN        EOS         EOS        EOS      EOS   \n",
       "7           NaN        EOS         EOS        EOS      EOS   \n",
       "8           NaN        EOS         EOS        EOS      EOS   \n",
       "9           NaN        EOS         EOS        EOS      EOS   \n",
       "\n",
       "  responsible_section seriousness_2 service_level service_type  \\\n",
       "2             Value 1       Value 1       Value 2      Value 1   \n",
       "3             Value 1       Value 1       Value 2      Value 1   \n",
       "4             Value 1       Value 1       Value 2      Value 1   \n",
       "5                 EOS           EOS           EOS          EOS   \n",
       "6                 EOS           EOS           EOS          EOS   \n",
       "7                 EOS           EOS           EOS          EOS   \n",
       "8                 EOS           EOS           EOS          EOS   \n",
       "9                 EOS           EOS           EOS          EOS   \n",
       "\n",
       "  support_section workgroup  case_elapsed_time  event_elapsed_time  \\\n",
       "2         Value 1   Value 1             4129.0                 8.0   \n",
       "3         Value 1   Value 1             9512.0              5383.0   \n",
       "4         Value 1   Value 1          4151033.0           4141521.0   \n",
       "5             EOS       EOS                NaN                 NaN   \n",
       "6             EOS       EOS                NaN                 NaN   \n",
       "7             EOS       EOS                NaN                 NaN   \n",
       "8             EOS       EOS                NaN                 NaN   \n",
       "9             EOS       EOS                NaN                 NaN   \n",
       "\n",
       "   day_in_week  seconds_in_day  \n",
       "2          3.0         31520.0  \n",
       "3          3.0         36903.0  \n",
       "4          2.0         31224.0  \n",
       "5          NaN             NaN  \n",
       "6          NaN             NaN  \n",
       "7          NaN             NaN  \n",
       "8          NaN             NaN  \n",
       "9          NaN             NaN  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f98f9641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5fab7e084a54ffea127c94d55c7b933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding data:   0%|          | 0/3091 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 3091 prefix/suffix pairs\n"
     ]
    }
   ],
   "source": [
    "from activity_pertubator import encode_single_dataframe\n",
    "\n",
    "# Load the trained encoder_decoder\n",
    "encoder_decoder = torch.load(\n",
    "    \"../encoded_data/data_encoder/helpdesk_encoder_decoder.pkl\",\n",
    "    weights_only=False\n",
    ")\n",
    "\n",
    "\n",
    "# Encode all prefix/suffix pairs\n",
    "encoded_data = {}\n",
    "for (case_id, prefix_len), (prefix_df, suffix_df) in tqdm(data.items(), desc=\"Encoding data\"):\n",
    "    # Encode prefix\n",
    "    encoded_prefix = encode_single_dataframe(\n",
    "        prefix_df, encoder_decoder, props[\"case_name\"], case_id\n",
    "    )\n",
    "    # Encode suffix\n",
    "    encoded_suffix = encode_single_dataframe(\n",
    "        suffix_df, encoder_decoder, props[\"case_name\"], case_id\n",
    "    )\n",
    "    # Store encoded pair\n",
    "    encoded_data[(case_id, prefix_len)] = (encoded_prefix, encoded_suffix)\n",
    "\n",
    "print(f\"Encoded {len(encoded_data)} prefix/suffix pairs\")\n",
    "\n",
    "torch.save(encoded_data, \"../encoded_data/helpdesk_redo_activity.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b63b845",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ad1528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Helper: decode encoded tensors back to readable events using Evaluation.case_to_readable\n",
    "# from model.dropout_uncertainty_enc_dec_LSTM.dropout_uncertainty_model import DropoutUncertaintyEncoderDecoderLSTM\n",
    "# from evaluation.evaluation import Evaluation\n",
    "\n",
    "# # Load trained model (needed only for its enc_feat definitions)\n",
    "# model_path = \"../src/notebooks/training_variational_dropout/Helpdesk/Helpdesk_full_no_grad_norm_new_2.pkl\"\n",
    "# model = DropoutUncertaintyEncoderDecoderLSTM.load(model_path, dropout=0.1)\n",
    "\n",
    "# # Load original dataset (provides encoders/decoders + metadata)\n",
    "# original_dataset_path = \"../encoded_data/helpdesk_all_5_test.pkl\"\n",
    "# original_dataset = torch.load(original_dataset_path, weights_only=False)\n",
    "\n",
    "# eval_helper = Evaluation(\n",
    "#     model=model,\n",
    "#     dataset=original_dataset,\n",
    "#     concept_name=props[\"concept_name\"],\n",
    "#     eos_value=\"EOS\",\n",
    "#     growing_num_values=[col for col in encoder_decoder.continuous_columns if \"elapsed\" in col]\n",
    "# )\n",
    "\n",
    "# def preview_encoded_pair(case_id: str, prefix_len: int, max_events: int = 5):\n",
    "#     encoded_prefix, encoded_suffix = encoded_data[(case_id, prefix_len)]\n",
    "#     readable_prefix = eval_helper.case_to_readable(encoded_prefix, prune_eos=False)\n",
    "#     readable_suffix = eval_helper.case_to_readable(encoded_suffix, prune_eos=False)\n",
    "#     print(f\"Decoded prefix for (case={case_id}, prefix_len={prefix_len}):\")\n",
    "#     for event in readable_prefix[:max_events]:\n",
    "#         print(event)\n",
    "#     print(\"\\nDecoded suffix (first events):\")\n",
    "#     for event in readable_suffix[:max_events]:\n",
    "#         print(event)\n",
    "\n",
    "# # Example preview\n",
    "# demo_case, demo_prefix_len = next(iter(encoded_data.keys()))\n",
    "# preview_encoded_pair(demo_case, demo_prefix_len)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "42144662",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(encoded_data, \"../encoded_data/helpdesk_augmented_loops.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68784fef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Probabilistic_Suffix_Prediction_U-ED-LSTM_-32bEAP25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
